{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nlpclass.config import model_config\n",
    "from nlpclass.data.data_utils import TranslationDataset, text_collate_func\n",
    "from nlpclass.models.evaluation_utils import bleu_eval, output_to_translations\n",
    "from nlpclass.models.models import DecoderRNN, EncoderCNN, EncoderRNN, TranslationModel\n",
    "from nlpclass.models.training_utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_DIR = osp.join(CURRENT_PATH, '..', 'data')\n",
    "MODEL_DIR = osp.join(CURRENT_PATH, '..','models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "Counted words:\n",
      "eng 6709\n",
      "vi 7674\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3562\n",
      "vi 3678\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3361\n",
      "vi 3518\n"
     ]
    }
   ],
   "source": [
    "data, data_loaders, max_length = load_data('vi', batch_size=2, subsample=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(\n",
    "    data['train'].input_lang.n_words,\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2).to(model_config.device)\n",
    "decoder = DecoderRNN(\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    output_size=data['train'].output_lang.n_words,\n",
    "    attention=False).to(model_config.device)\n",
    "translation_model = TranslationModel(encoder, decoder, teacher_forcing_ratio=0.5).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(\n",
    "    data['train'].input_lang.n_words,\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True).to(model_config.device)\n",
    "if encoder.bidirectional:\n",
    "    multiplier = 2\n",
    "else:\n",
    "    multiplier = 1\n",
    "decoder = DecoderRNN(\n",
    "    embedding_size=100,\n",
    "    hidden_size=multiplier * 128,\n",
    "    output_size=data['train'].output_lang.n_words,\n",
    "    attention=True).to(model_config.device)\n",
    "translation_model = TranslationModel(encoder, decoder, teacher_forcing_ratio=0.5).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, target, criterion):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_flat = target.view(-1, 1).squeeze()\n",
    "    return criterion(logits_flat, target_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, data_loaders, dataset_type='dev', max_batches=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        original_strings = []\n",
    "        translated_strings = []\n",
    "        for i, batch in enumerate(data_loaders[dataset_type]):\n",
    "            if i > max_batches:\n",
    "                break\n",
    "            logits = translation_model(batch)\n",
    "            epoch_loss = calc_loss(logits, batch['target'], criterion)\n",
    "            original = output_to_translations(batch['target'], data['train'])\n",
    "            translations = output_to_translations(model.greedy(batch), data['train'])\n",
    "            original_strings.extend(original)\n",
    "            translated_strings.extend(translations)\n",
    "        bleu = bleu_eval(original_strings, translated_strings)\n",
    "        model.train()\n",
    "        print(epoch_loss)\n",
    "        print(bleu)\n",
    "        \n",
    "        return original_strings, translated_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9524e629748f4dcca2dfbe618bdb3e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020137496824000926 4.634426597858985 5.626060962677002\n",
      "0.013517843523675067 107.33386255863229 12.328470230102539\n",
      "0.48494471422290086 222.0302262670534 15.045141220092773\n",
      "30.391699882161944 214.54025889667741 10.102069854736328\n",
      "0.4999193563602579 3.259551601435442 7.100986003875732\n",
      "2.4976484312463882 3.00420777392432 6.747938632965088\n",
      "0.15120426317697222 3.2237099564328795 6.767729759216309\n",
      "0.21003912480436002 4.4115080652282606 6.9724836349487305\n",
      "0.9085347633240615 2.9892800583846686 7.226278305053711\n",
      "152.3248277138639 545.550505056196 14.625187873840332\n",
      "0.1274045836639269 11.903812258313298 7.20048189163208\n",
      "1.8443979550091911 3.2978176538244597 7.7557477951049805\n",
      "0.20664004741306952 1.3343666424972969 7.667934417724609\n",
      "0.092596585404913 1.8405205211811728 7.21620512008667\n",
      "0.09017671927511317 2.118393123799774 7.055311679840088\n",
      "0.049907650533802174 2.6228875314903672 6.981229305267334\n",
      "0.043321533647958016 1.7091123785435929 6.3538594245910645\n",
      "0.05181700890728001 3.7120551973889384 7.090948581695557\n",
      "0.20796098360685672 1.8055412496781007 7.048529148101807\n",
      "0.9110155289459926 1.748875070426441 7.1643171310424805\n",
      "2.8280947829732397 7.8503547071750495 7.384737968444824\n",
      "0.23380403999248298 1.999484718087811 7.963827133178711\n",
      "0.1376688437117567 2.212421890081686 8.771328926086426\n",
      "0.03259098672221648 2.9932664247854786 7.726367950439453\n",
      "0.04461501324438778 1.6839822832929001 7.662903308868408\n",
      "0.11285005142410927 1.445123955352311 7.865818023681641\n",
      "0.22477391576745184 2.2538497772215886 7.896220684051514\n",
      "0.04858772812064578 1.4452492030853004 7.437581539154053\n",
      "1.219557876151186 2.0039860826566387 7.101127624511719\n",
      "0.10784089224314782 2.02796794876168 7.483133316040039\n",
      "0.10193711333855533 2.133690173171565 7.481202125549316\n",
      "0.2031128351061299 2.5573899996927585 8.005924224853516\n",
      "0.03626290744766211 1.5408837844027592 7.3589019775390625\n",
      "0.0783429999043677 1.2552001136704731 7.982166290283203\n",
      "0.0506000451415613 1.3539624567284763 7.254293441772461\n",
      "0.08890578325042703 1.0708245462660735 6.997925758361816\n",
      "0.04042071189537209 1.2998046885105672 7.367826461791992\n",
      "0.03694039132190142 1.1914103728244685 6.816606044769287\n",
      "0.16812081041920995 1.2265056491895137 7.1356425285339355\n",
      "1.4596843672240711 3.1853926906878547 7.734482765197754\n",
      "0.07023060613558928 1.7277653452921689 7.503685474395752\n",
      "0.4513098962380435 1.7854394802825801 7.732008934020996\n",
      "0.04871513316776985 1.8663263457317079 7.53017520904541\n",
      "2.0627190289447985 8.876305704169638 8.662248611450195\n",
      "0.17124857653425407 2.8269849228711674 7.951765537261963\n",
      "0.09270624037795977 1.5739708960718932 7.460692882537842\n",
      "0.058734133959149004 1.0553438261758985 7.55527400970459\n",
      "0.021366727394709694 1.6069325412729725 7.238954544067383\n",
      "0.03227515865949339 1.3586298748815402 7.269662380218506\n",
      "0.066673969261789 1.8226140980232388 7.548828125\n",
      "0.020654662146753447 1.2682072118028729 7.155585765838623\n",
      "0.06345114365691724 1.4105760787495554 7.664954662322998\n",
      "0.017975063898237196 1.4469387896943342 6.948233127593994\n",
      "0.013831889613888669 1.8683180591110649 6.894522190093994\n",
      "0.014155585506955648 2.7885533423455007 7.2588982582092285\n",
      "0.010904808855845185 0.8886313965481671 6.889466762542725\n",
      "0.014053307265470655 1.2534664602464263 7.0751190185546875\n",
      "0.04337604864340467 1.6729891446839942 7.2519941329956055\n",
      "0.03004109135023022 2.1743042234802314 7.648247718811035\n",
      "0.035302233566274696 1.522331718701486 7.261192321777344\n",
      "0.021471433479847594 1.348448790446679 6.830083847045898\n",
      "0.029307488745349178 1.3030187080832076 7.2047810554504395\n",
      "0.020127624606239406 1.2037977964142903 7.7663798332214355\n",
      "0.01146575395914053 1.1332685212270346 7.214675426483154\n",
      "0.009500486201233267 1.0210787006562205 7.479816436767578\n",
      "0.011647902319307585 1.1657376110830437 7.68729829788208\n",
      "0.007624268291269746 0.9293115678475591 7.171767234802246\n",
      "0.008604437847731202 0.8700395167514886 7.350834846496582\n",
      "0.025076757785352528 1.960890820812781 7.257453918457031\n",
      "0.006649680341314787 1.0191755828661608 7.41972541809082\n",
      "0.006414298212981109 1.0308616089894782 7.5253729820251465\n",
      "0.009874363045343131 0.9364811354486527 7.632758617401123\n",
      "0.015494388787853952 0.7974645251170854 7.46138858795166\n",
      "0.009523550090818591 0.8499069348507117 7.320221900939941\n",
      "0.012814100259683069 2.899456434541623 6.944215774536133\n",
      "0.013778871751732648 1.1487297584128442 6.936977863311768\n",
      "0.010948672145651291 1.2251933060416218 6.7367424964904785\n",
      "0.00918174493133339 1.3792017898129683 7.493274688720703\n",
      "0.00511148957144553 0.9758943832817027 7.102034091949463\n",
      "0.00762706548374503 1.4601936587628221 7.339620113372803\n",
      "0.007379852673956333 1.0297198538210874 7.341565132141113\n",
      "0.00688145887972025 0.7430937533823243 7.035190582275391\n",
      "0.009548361296953922 1.0228868635439479 6.8818559646606445\n",
      "0.012222313289390191 1.0369117212592383 6.625770568847656\n",
      "0.006463366029483115 2.6122366144494698 7.094444274902344\n",
      "0.007549062080705328 1.8183642571123981 7.278975486755371\n",
      "0.01380244656466826 1.392625205991247 7.132708549499512\n",
      "0.006464518216531565 1.4332305081522063 7.06847620010376\n",
      "0.00499444171694996 1.0591883755375586 7.100709438323975\n",
      "0.014156519181256447 1.3142489821184034 7.329854488372803\n",
      "0.012029951160526414 1.1501570669082202 7.1112518310546875\n",
      "0.005232909741257552 1.0712321413527552 7.510432720184326\n",
      "0.008258826703066796 1.3057162660365804 7.103081226348877\n",
      "0.5470231422794676 2.1103092582365544 7.400974750518799\n",
      "0.00417182600183152 1.8724304077411311 7.490915775299072\n",
      "0.0076891714670975775 1.2296296966486802 6.9921875\n",
      "0.007488865957682898 1.7101112958751774 7.3624267578125\n",
      "0.0036182867243838377 0.9107277524916996 7.044229507446289\n",
      "0.00732691065270949 1.1343580152362245 7.4780592918396\n",
      "0.0047473763362756325 0.9587714489055406 7.560918807983398\n",
      "0.004755993729834032 1.1076554926543454 6.939793586730957\n",
      "0.006777153448525229 0.955156782220871 6.822502613067627\n",
      "0.008072826040832981 0.8262934197284802 7.21639347076416\n",
      "0.0294978988200696 0.9157135068006153 7.320426940917969\n",
      "0.004921066261107006 0.5924536988727432 6.922159671783447\n",
      "0.00415190083083188 1.4539063244437909 7.18754768371582\n",
      "0.005429011601455728 2.2982177746226595 7.754169464111328\n",
      "0.004661196378466313 0.956704295243869 7.346693992614746\n",
      "0.00709721073881194 1.2129395178468187 6.7563042640686035\n",
      "0.003430405730351191 1.3318036630270047 6.818308353424072\n",
      "0.007688922247889521 0.9815049343757137 6.988131523132324\n",
      "0.00456957547958676 1.0972104096954987 7.258789539337158\n",
      "0.0035543007386426646 1.3996458089404396 7.558762550354004\n",
      "0.003012549313498772 0.9853336907338305 7.323357582092285\n",
      "0.005908098146533832 1.0929061294046696 7.174319267272949\n",
      "0.0046925261271630195 0.7702753105992349 7.122283935546875\n",
      "0.004158081557365007 0.8950999499781793 7.0856170654296875\n",
      "0.006319959951681351 1.2084010797238631 7.4740824699401855\n",
      "0.0032353415889064927 0.9032222147198379 7.060581207275391\n",
      "0.0058733319596770694 1.425054647502017 7.355360507965088\n",
      "0.003714329258274577 1.119093889768916 7.155043125152588\n",
      "0.004430769907588497 1.0526702805157038 7.45957088470459\n",
      "0.008337501569686177 0.726979291927882 6.993529796600342\n",
      "0.004580383587434443 0.8491760926877999 7.041871070861816\n",
      "0.009429380265153888 1.1845449159797545 7.527159690856934\n",
      "0.07422819680298416 0.7281358654314403 6.963061809539795\n",
      "0.008878298531474114 1.001361785971012 7.184299945831299\n",
      "0.005619501125515547 0.8533237425861903 7.319461822509766\n",
      "0.007087209788171883 0.8007481298369099 6.5541768074035645\n",
      "0.005310301425291619 0.9405438735805125 7.249414443969727\n",
      "0.005648277322494612 1.8831013577672047 7.743675231933594\n",
      "0.002808695401218057 1.2316304006328866 7.4112348556518555\n",
      "0.013222561213911306 0.8362836100474917 7.138485908508301\n",
      "0.006332684102213908 1.1547910033738231 7.745028018951416\n",
      "0.007082008807000092 1.6111166092347065 6.561421871185303\n",
      "0.004530105935858972 1.1407792307951936 6.634211540222168\n",
      "0.013610497779975987 0.938183743405145 7.48769474029541\n",
      "0.0069798284144457975 2.219368727012516 7.343203067779541\n",
      "0.005683131251937799 1.3046287693842795 7.1453142166137695\n",
      "tensor(6.6414, device='cuda:0')\n",
      "0.2255507636266751\n",
      "0.009806845594174702 1.0386434217488856 6.207946300506592\n",
      "0.07693514127371348 1.4551972790077239 7.139617443084717\n",
      "0.009213909500232513 1.1204594636463021 6.615677356719971\n",
      "0.005435586956562488 1.441510816261011 6.838636875152588\n",
      "0.005014495135898931 1.2330128893750685 6.955657482147217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005066078404559122 0.9399914171559608 5.947779655456543\n",
      "0.009557955448134833 1.458418145081454 6.538112163543701\n",
      "0.007028175551751372 1.1404207871505092 6.5298357009887695\n",
      "0.01995761000481369 0.9203472766135045 6.701842308044434\n",
      "0.007616119358221397 1.320857260665912 5.865251064300537\n",
      "0.010199047609220614 1.0271163162594674 5.690906524658203\n",
      "0.010846928040286403 0.7417836377620955 5.936854362487793\n",
      "0.0058806240524687595 0.9846039297058174 6.61268424987793\n",
      "0.005462618259263431 1.0709079491629383 5.701878070831299\n",
      "0.004122173196795206 0.9372017552358015 6.307410717010498\n",
      "0.002715935338956546 0.6911030680388788 6.213427543640137\n",
      "0.006173176831278642 1.1854937319657601 6.815855979919434\n",
      "0.003774298579827752 1.3353050030469464 7.183228492736816\n",
      "0.004010815344478557 1.870776661222798 7.045688152313232\n",
      "0.0036580602171487615 1.2312393664152097 6.1077961921691895\n",
      "0.047271054072556606 1.0185015957753891 5.933896541595459\n",
      "0.009984990338158698 1.051970161676569 6.969225883483887\n",
      "0.007394928448926138 1.0095489072551125 6.357835292816162\n",
      "0.008584750152741157 1.2342626181747427 6.100264072418213\n",
      "0.0038002977656367335 0.8519606414858655 5.928931713104248\n",
      "0.006275059684538686 1.1858078335933866 7.019402980804443\n",
      "0.008305415823074088 1.2311652213164797 6.846307277679443\n",
      "0.008449720044970667 0.6626203679983964 6.400295734405518\n",
      "0.011789393222220074 0.6921687054912032 6.306027889251709\n",
      "0.012555411491950564 0.830691984770838 5.845983982086182\n",
      "0.01537900095525979 0.8962618204899354 6.877438545227051\n",
      "0.029698388501185228 1.154438998509913 6.8475494384765625\n",
      "0.01959192482032273 1.3840175718310828 6.888199806213379\n",
      "0.015485546771211573 0.7663570199741085 6.870859146118164\n",
      "0.005884596874019348 1.0709761450673523 6.234323024749756\n",
      "0.004537779519752295 0.8767182515579411 6.298633098602295\n",
      "0.0041954286596966985 0.6497085086572671 6.177331924438477\n",
      "0.008375459892928424 0.6478743926122722 6.217463970184326\n",
      "0.007138871077996039 0.6857576474012217 6.26557731628418\n",
      "0.020446113816078307 0.7944909867040866 6.81988525390625\n",
      "0.008077593214602788 0.6859045104399772 6.40265417098999\n",
      "0.05174761563020755 1.403124874107135 6.824069976806641\n",
      "0.011719426610002765 0.8487581621013378 7.173518180847168\n",
      "0.017188307134522293 0.8253788952919687 6.685458660125732\n",
      "0.013827501164163316 3.780371556765738 7.403214454650879\n",
      "0.013763317856021832 0.7511335426432374 6.796042442321777\n",
      "0.009263488335236002 0.6963619776287014 6.086002826690674\n",
      "0.011449734958942288 0.7869417795974304 6.6773552894592285\n",
      "0.012663913470799915 1.065434361303888 6.054440498352051\n",
      "0.013054874803073475 0.9054618037738933 7.375857830047607\n",
      "0.013124528225160548 0.8533039280661774 6.872260093688965\n",
      "0.008842386224043363 0.905360443641805 6.01541805267334\n",
      "0.005615690878105633 0.8314655202167255 6.530973434448242\n",
      "0.6379726825100546 1.1466309714733456 6.218494415283203\n",
      "0.006721944012018353 0.6294542260213346 6.5186285972595215\n",
      "0.004665191878755205 0.6735276551684144 5.920233726501465\n",
      "0.016059963955159334 0.8081010291797256 6.2575507164001465\n",
      "0.005501814490444907 0.9005778191129478 6.788167476654053\n",
      "0.007033842262970771 0.8814490226968498 6.999979496002197\n",
      "2.313167047927691 2.8600215199094787 6.889101028442383\n",
      "0.006993397367432743 0.9157620076303752 6.16946268081665\n",
      "0.009023646190505016 0.8193299622908636 5.992068767547607\n",
      "0.008363835952947101 0.6902596961265813 7.023804187774658\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-257ee7664189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mencoder_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(25)):\n",
    "    for batch in data_loaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        logits = translation_model(batch)\n",
    "        loss = calc_loss(logits, batch['target'], criterion)\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_norm = 0\n",
    "        for p in translation_model.encoder.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            encoder_norm += param_norm.item() ** 2\n",
    "        decoder_norm = 0\n",
    "        for p in translation_model.decoder.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            decoder_norm += param_norm.item() ** 2\n",
    "            \n",
    "        print(encoder_norm, decoder_norm, loss.item())\n",
    "            \n",
    "        clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                                   translation_model.parameters()), 5.0)\n",
    "        \n",
    "        optimizer.step() \n",
    "    original, translation = evaluate(translation_model, data, data_loaders, dataset_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([37, 37], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([234,  33], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([411,  13], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([33, 33], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([704,  13], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([12, 33], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([ 7, 13], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([10, 33], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([12, 13], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([ 7, 64], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([10, 37], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([145,  13], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([21, 64], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 37], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 7], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([ 19, 126], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 7], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([ 19, 126], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 7], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([19, 19], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "tensor([1, 1], device='cuda:0') torch.Size([2])\n",
      "torch.Size([2, 1, 100]) torch.Size([2, 1, 256])\n",
      "0.08800825606880082\n",
      "[('not transparent avaz there spread animal polyglottos emcees animal polyglottos emcees proposed going was', 'and she said quot yeah but shes why im here quot because she was accused of stealing two diapers and an iron for her baby and still had been in prison .'), ('not there kingdom there kingdom there kingdom there kingdom answered not kingdom answered not polyglottos books polyglottos books polyglottos was', 'is it thinking lamprey eel thoughts sitting there in its nutrient medium ?')]\n"
     ]
    }
   ],
   "source": [
    "#x = next(iter(data_loaders['train']))\n",
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.greedy(x), data['train'])\n",
    "print(bleu_eval(original, translations))\n",
    "print(list(zip(translations[0:2], original[0:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.beam_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93794316]\n",
      "[0.44554293]\n",
      "[0.28218982]\n",
      "[0.20106924]\n",
      "[0.15281946]\n",
      "[0.12098749]\n",
      "[0.09852279]\n",
      "[0.08190073]\n",
      "[0.06916371]\n",
      "[0.05913762]\n",
      "[0.05107569]\n",
      "[0.0444804]\n",
      "[0.03900763]\n",
      "[0.03441185]\n",
      "[0.0305132]\n",
      "[0.02717704]\n",
      "[0.02430056]\n",
      "[0.02180397]\n",
      "[0.01962445]\n",
      "[0.01771186]\n",
      "[0.01602574]\n",
      "[0.01453308]\n",
      "[0.01320674]\n",
      "[0.01202417]\n",
      "[0.01096653]\n",
      "[0.01001796]\n",
      "[0.00916498]\n",
      "[0.00839616]\n",
      "[0.00770165]\n",
      "[0.00707299]\n",
      "[0.00650288]\n",
      "[0.00598495]\n",
      "[0.00551366]\n",
      "[0.00508414]\n",
      "[0.00469215]\n",
      "[0.00433391]\n",
      "[0.00400612]\n",
      "[0.00370583]\n",
      "[0.00343042]\n",
      "[0.00317757]\n",
      "[0.0029452]\n",
      "[0.00273144]\n",
      "[0.00253464]\n",
      "[0.00235328]\n",
      "[0.00218604]\n",
      "[0.00203168]\n",
      "[0.00188911]\n",
      "[0.00175735]\n",
      "[0.00163548]\n",
      "[0.0015227]\n",
      "[0.00141827]\n",
      "[0.0013215]\n",
      "[0.0012318]\n",
      "[0.00114859]\n",
      "[0.00107137]\n",
      "[0.00099967]\n",
      "[0.00093306]\n",
      "[0.00087117]\n",
      "[0.00081362]\n",
      "[0.00076009]\n",
      "[0.00071028]\n",
      "[0.00066391]\n",
      "[0.00062073]\n",
      "[0.00058051]\n",
      "[0.00054302]\n",
      "[0.00050808]\n",
      "[0.00047549]\n",
      "[0.00044509]\n",
      "[0.00041673]\n",
      "[0.00039025]\n",
      "[0.00036554]\n",
      "[0.00034245]\n",
      "[0.00032089]\n",
      "[0.00030074]\n",
      "[0.0002819]\n",
      "[0.0002643]\n",
      "[0.00024783]\n",
      "[0.00023243]\n",
      "[0.00021803]\n",
      "[0.00020454]\n",
      "[0.00019193]\n",
      "[0.00018011]\n",
      "[0.00016905]\n",
      "[0.0001587]\n",
      "[0.000149]\n",
      "[0.00013991]\n",
      "[0.00013139]\n",
      "[0.00012341]\n",
      "[0.00011592]\n",
      "[0.00010891]\n",
      "[0.00010233]\n",
      "[9.61623549e-05]\n",
      "[9.03759772e-05]\n",
      "[8.49476049e-05]\n",
      "[7.98543216e-05]\n",
      "[7.50747395e-05]\n",
      "[7.05888928e-05]\n",
      "[6.6378138e-05]\n",
      "[6.24250623e-05]\n",
      "[0.95554713]\n",
      "[0.46330637]\n",
      "[0.29951816]\n",
      "[0.21783647]\n",
      "[0.16899222]\n",
      "[0.13656255]\n",
      "[0.11350918]\n",
      "[0.09631306]\n",
      "[0.08301926]\n",
      "[0.07245486]\n",
      "[0.06387354]\n",
      "[0.0567778]\n",
      "[0.05082327]\n",
      "[0.04576401]\n",
      "[0.04141971]\n",
      "[0.03765515]\n",
      "[0.034367]\n",
      "[0.03147489]\n",
      "[0.0289154]\n",
      "[0.02663784]\n",
      "[0.02460118]\n",
      "[0.02277187]\n",
      "[0.02112222]\n",
      "[0.01962919]\n",
      "[0.01827342]\n",
      "[0.01703855]\n",
      "[0.01591067]\n",
      "[0.01487785]\n",
      "[0.01392985]\n",
      "[0.01305778]\n",
      "[0.01225392]\n",
      "[0.01151153]\n",
      "[0.01082468]\n",
      "[0.01018817]\n",
      "[0.00959739]\n",
      "[0.00904826]\n",
      "[0.00853713]\n",
      "[0.00806076]\n",
      "[0.00761625]\n",
      "[0.00720099]\n",
      "[0.00681262]\n",
      "[0.00644904]\n",
      "[0.00610832]\n",
      "[0.00578874]\n",
      "[0.00548871]\n",
      "[0.0052068]\n",
      "[0.00494171]\n",
      "[0.00469224]\n",
      "[0.0044573]\n",
      "[0.00423588]\n",
      "[0.00402707]\n",
      "[0.00383003]\n",
      "[0.00364398]\n",
      "[0.0034682]\n",
      "[0.00330204]\n",
      "[0.00314487]\n",
      "[0.00299614]\n",
      "[0.00285532]\n",
      "[0.00272193]\n",
      "[0.00259552]\n",
      "[0.00247566]\n",
      "[0.00236198]\n",
      "[0.0022541]\n",
      "[0.00215169]\n",
      "[0.00205444]\n",
      "[0.00196204]\n",
      "[0.00187423]\n",
      "[0.00179075]\n",
      "[0.00171136]\n",
      "[0.00163583]\n",
      "[0.00156396]\n",
      "[0.00149553]\n",
      "[0.00143038]\n",
      "[0.00136833]\n",
      "[0.0013092]\n",
      "[0.00125285]\n",
      "[0.00119914]\n",
      "[0.00114792]\n",
      "[0.00109907]\n",
      "[0.00105247]\n",
      "[0.001008]\n",
      "[0.00096555]\n",
      "[0.00092504]\n",
      "[0.00088635]\n",
      "[0.0008494]\n",
      "[0.0008141]\n",
      "[0.00078037]\n",
      "[0.00074814]\n",
      "[0.00071734]\n",
      "[0.00068789]\n",
      "[0.00065973]\n",
      "[0.0006328]\n",
      "[0.00060704]\n",
      "[0.00058239]\n",
      "[0.00055881]\n",
      "[0.00053625]\n",
      "[0.00051465]\n",
      "[0.00049397]\n",
      "[0.00047418]\n",
      "[0.98332728]\n",
      "[0.49166176]\n",
      "[0.32777326]\n",
      "[0.24582901]\n",
      "[0.19666245]\n",
      "[0.16388475]\n",
      "[0.14047211]\n",
      "[0.12291263]\n",
      "[0.10925525]\n",
      "[0.09832935]\n",
      "[0.08938998]\n",
      "[0.0819405]\n",
      "[0.0756371]\n",
      "[0.07023418]\n",
      "[0.06555165]\n",
      "[0.06145444]\n",
      "[0.05783925]\n",
      "[0.05462575]\n",
      "[0.05175051]\n",
      "[0.0491628]\n",
      "[0.04682154]\n",
      "[0.04469311]\n",
      "[0.04274977]\n",
      "[0.04096838]\n",
      "[0.03932949]\n",
      "[0.03781667]\n",
      "[0.03641592]\n",
      "[0.03511521]\n",
      "[0.03390422]\n",
      "[0.03277395]\n",
      "[0.0317166]\n",
      "[0.03072534]\n",
      "[0.02979416]\n",
      "[0.02891775]\n",
      "[0.02809142]\n",
      "[0.027311]\n",
      "[0.02657276]\n",
      "[0.02587338]\n",
      "[0.02520987]\n",
      "[0.02457952]\n",
      "[0.02397993]\n",
      "[0.02340889]\n",
      "[0.02286441]\n",
      "[0.02234468]\n",
      "[0.02184805]\n",
      "[0.02137301]\n",
      "[0.02091819]\n",
      "[0.02048231]\n",
      "[0.02006423]\n",
      "[0.01966287]\n",
      "[0.01927725]\n",
      "[0.01890646]\n",
      "[0.01854966]\n",
      "[0.01820608]\n",
      "[0.017875]\n",
      "[0.01755573]\n",
      "[0.01724767]\n",
      "[0.01695023]\n",
      "[0.01666288]\n",
      "[0.0163851]\n",
      "[0.01611643]\n",
      "[0.01585643]\n",
      "[0.01560468]\n",
      "[0.0153608]\n",
      "[0.01512442]\n",
      "[0.0148952]\n",
      "[0.01467283]\n",
      "[0.014457]\n",
      "[0.01424742]\n",
      "[0.01404384]\n",
      "[0.01384598]\n",
      "[0.01365362]\n",
      "[0.01346654]\n",
      "[0.01328451]\n",
      "[0.01310733]\n",
      "[0.01293482]\n",
      "[0.01276678]\n",
      "[0.01260306]\n",
      "[0.01244348]\n",
      "[0.01228789]\n",
      "[0.01213614]\n",
      "[0.01198809]\n",
      "[0.01184361]\n",
      "[0.01170257]\n",
      "[0.01156485]\n",
      "[0.01143033]\n",
      "[0.0112989]\n",
      "[0.01117047]\n",
      "[0.01104491]\n",
      "[0.01092215]\n",
      "[0.01080209]\n",
      "[0.01068463]\n",
      "[0.0105697]\n",
      "[0.01045722]\n",
      "[0.0103471]\n",
      "[0.01023928]\n",
      "[0.01013368]\n",
      "[0.01003024]\n",
      "[0.00992889]\n",
      "[0.9817924]\n",
      "[0.48480244]\n",
      "[0.31918955]\n",
      "[0.23642046]\n",
      "[0.18678852]\n",
      "[0.15372484]\n",
      "[0.13012849]\n",
      "[0.112449]\n",
      "[0.09871387]\n",
      "[0.08773964]\n",
      "[0.07877316]\n",
      "[0.07131237]\n",
      "[0.06500966]\n",
      "[0.05961675]\n",
      "[0.05495158]\n",
      "[0.0508776]\n",
      "[0.04729038]\n",
      "[0.04410871]\n",
      "[0.04126847]\n",
      "[0.03871838]\n",
      "[0.0364169]\n",
      "[0.03433007]\n",
      "[0.03242983]\n",
      "[0.03069279]\n",
      "[0.02909931]\n",
      "[0.02763278]\n",
      "[0.02627903]\n",
      "[0.02502592]\n",
      "[0.02386301]\n",
      "[0.02278123]\n",
      "[0.02177268]\n",
      "[0.02083045]\n",
      "[0.01994848]\n",
      "[0.01912141]\n",
      "[0.01834451]\n",
      "[0.01761354]\n",
      "[0.01692476]\n",
      "[0.01627481]\n",
      "[0.01566066]\n",
      "[0.0150796]\n",
      "[0.01452918]\n",
      "[0.01400718]\n",
      "[0.0135116]\n",
      "[0.0130406]\n",
      "[0.01259253]\n",
      "[0.01216586]\n",
      "[0.0117592]\n",
      "[0.01137128]\n",
      "[0.01100094]\n",
      "[0.01064709]\n",
      "[0.01030875]\n",
      "[0.009985]\n",
      "[0.00967499]\n",
      "[0.00937795]\n",
      "[0.00909314]\n",
      "[0.0088199]\n",
      "[0.0085576]\n",
      "[0.00830566]\n",
      "[0.00806353]\n",
      "[0.00783071]\n",
      "[0.00760672]\n",
      "[0.00739113]\n",
      "[0.00718352]\n",
      "[0.0069835]\n",
      "[0.0067907]\n",
      "[0.00660479]\n",
      "[0.00642545]\n",
      "[0.00625237]\n",
      "[0.00608526]\n",
      "[0.00592387]\n",
      "[0.00576794]\n",
      "[0.00561722]\n",
      "[0.0054715]\n",
      "[0.00533055]\n",
      "[0.00519419]\n",
      "[0.00506222]\n",
      "[0.00493445]\n",
      "[0.00481072]\n",
      "[0.00469086]\n",
      "[0.00457472]\n",
      "[0.00446216]\n",
      "[0.00435303]\n",
      "[0.00424719]\n",
      "[0.00414454]\n",
      "[0.00404494]\n",
      "[0.00394827]\n",
      "[0.00385444]\n",
      "[0.00376334]\n",
      "[0.00367486]\n",
      "[0.00358892]\n",
      "[0.00350542]\n",
      "[0.00342427]\n",
      "[0.0033454]\n",
      "[0.00326873]\n",
      "[0.00319417]\n",
      "[0.00312166]\n",
      "[0.00305113]\n",
      "[0.0029825]\n",
      "[0.00291573]\n",
      "[0.988082]\n",
      "[0.4891055]\n",
      "[0.32281288]\n",
      "[0.23969097]\n",
      "[0.18983716]\n",
      "[0.15661723]\n",
      "[0.13290224]\n",
      "[0.11512772]\n",
      "[0.10131342]\n",
      "[0.09027116]\n",
      "[0.08124486]\n",
      "[0.07373045]\n",
      "[0.06737897]\n",
      "[0.06194115]\n",
      "[0.05723419]\n",
      "[0.05312102]\n",
      "[0.04949679]\n",
      "[0.04627996]\n",
      "[0.04340617]\n",
      "[0.04082391]\n",
      "[0.03849151]\n",
      "[0.03637484]\n",
      "[0.03444574]\n",
      "[0.03268072]\n",
      "[0.03106007]\n",
      "[0.02956709]\n",
      "[0.02818758]\n",
      "[0.02690934]\n",
      "[0.02572188]\n",
      "[0.02461608]\n",
      "[0.02358403]\n",
      "[0.02261879]\n",
      "[0.02171426]\n",
      "[0.02086506]\n",
      "[0.02006642]\n",
      "[0.01931413]\n",
      "[0.01860439]\n",
      "[0.01793383]\n",
      "[0.01729943]\n",
      "[0.01669844]\n",
      "[0.01612841]\n",
      "[0.01558711]\n",
      "[0.01507253]\n",
      "[0.01458282]\n",
      "[0.01411631]\n",
      "[0.01367147]\n",
      "[0.01324692]\n",
      "[0.01284136]\n",
      "[0.01245363]\n",
      "[0.01208263]\n",
      "[0.01172738]\n",
      "[0.01138694]\n",
      "[0.01106049]\n",
      "[0.01074721]\n",
      "[0.0104464]\n",
      "[0.01015736]\n",
      "[0.00987947]\n",
      "[0.00961214]\n",
      "[0.00935482]\n",
      "[0.00910701]\n",
      "[0.00886823]\n",
      "[0.00863803]\n",
      "[0.00841599]\n",
      "[0.00820173]\n",
      "[0.00799487]\n",
      "[0.00779508]\n",
      "[0.00760202]\n",
      "[0.0074154]\n",
      "[0.00723492]\n",
      "[0.00706032]\n",
      "[0.00689134]\n",
      "[0.00672774]\n",
      "[0.00656929]\n",
      "[0.00641578]\n",
      "[0.00626699]\n",
      "[0.00612275]\n",
      "[0.00598286]\n",
      "[0.00584715]\n",
      "[0.00571547]\n",
      "[0.00558764]\n",
      "[0.00546352]\n",
      "[0.00534298]\n",
      "[0.00522587]\n",
      "[0.00511207]\n",
      "[0.00500146]\n",
      "[0.00489392]\n",
      "[0.00478934]\n",
      "[0.00468762]\n",
      "[0.00458864]\n",
      "[0.00449233]\n",
      "[0.00439858]\n",
      "[0.0043073]\n",
      "[0.00421842]\n",
      "[0.00413185]\n",
      "[0.00404751]\n",
      "[0.00396534]\n",
      "[0.00388525]\n",
      "[0.00380719]\n",
      "[0.00373108]\n",
      "[0.98958895]\n",
      "[0.49355312]\n",
      "[0.32820992]\n",
      "[0.24553988]\n",
      "[0.19593909]\n",
      "[0.16287293]\n",
      "[0.13925512]\n",
      "[0.12154253]\n",
      "[0.10776676]\n",
      "[0.09674675]\n",
      "[0.08773094]\n",
      "[0.08021827]\n",
      "[0.07386186]\n",
      "[0.06841394]\n",
      "[0.06369281]\n",
      "[0.05956221]\n",
      "[0.05591791]\n",
      "[0.05267886]\n",
      "[0.04978108]\n",
      "[0.04717338]\n",
      "[0.04481432]\n",
      "[0.04266998]\n",
      "[0.04071237]\n",
      "[0.03891813]\n",
      "[0.03726768]\n",
      "[0.0357444]\n",
      "[0.03433418]\n",
      "[0.0330249]\n",
      "[0.03180611]\n",
      "[0.03066877]\n",
      "[0.029605]\n",
      "[0.02860789]\n",
      "[0.02767138]\n",
      "[0.02679014]\n",
      "[0.02595941]\n",
      "[0.025175]\n",
      "[0.02443314]\n",
      "[0.02373048]\n",
      "[0.023064]\n",
      "[0.02243098]\n",
      "[0.02182898]\n",
      "[0.02125578]\n",
      "[0.02070937]\n",
      "[0.02018793]\n",
      "[0.01968979]\n",
      "[0.01921342]\n",
      "[0.01875745]\n",
      "[0.01832059]\n",
      "[0.01790168]\n",
      "[0.01749963]\n",
      "[0.01711346]\n",
      "[0.01674224]\n",
      "[0.01638514]\n",
      "[0.01604137]\n",
      "[0.01571019]\n",
      "[0.01539094]\n",
      "[0.01508299]\n",
      "[0.01478575]\n",
      "[0.01449868]\n",
      "[0.01422127]\n",
      "[0.01395304]\n",
      "[0.01369355]\n",
      "[0.01344238]\n",
      "[0.01319914]\n",
      "[0.01296347]\n",
      "[0.01273503]\n",
      "[0.01251348]\n",
      "[0.01229853]\n",
      "[0.01208988]\n",
      "[0.01188727]\n",
      "[0.01169044]\n",
      "[0.01149915]\n",
      "[0.01131317]\n",
      "[0.01113229]\n",
      "[0.01095631]\n",
      "[0.01078502]\n",
      "[0.01061825]\n",
      "[0.01045582]\n",
      "[0.01029757]\n",
      "[0.01014333]\n",
      "[0.00999297]\n",
      "[0.00984634]\n",
      "[0.00970331]\n",
      "[0.00956374]\n",
      "[0.00942751]\n",
      "[0.00929451]\n",
      "[0.00916463]\n",
      "[0.00903775]\n",
      "[0.00891379]\n",
      "[0.00879263]\n",
      "[0.00867419]\n",
      "[0.00855838]\n",
      "[0.00844512]\n",
      "[0.00833431]\n",
      "[0.00822589]\n",
      "[0.00811978]\n",
      "[0.00801591]\n",
      "[0.00791421]\n",
      "[0.00781462]\n",
      "[0.96444044]\n",
      "[0.47421195]\n",
      "[0.31089112]\n",
      "[0.22929609]\n",
      "[0.18039052]\n",
      "[0.14782897]\n",
      "[0.12460625]\n",
      "[0.1072198]\n",
      "[0.09372372]\n",
      "[0.08295052]\n",
      "[0.07415723]\n",
      "[0.06684856]\n",
      "[0.0606816]\n",
      "[0.05541143]\n",
      "[0.05085847]\n",
      "[0.04688799]\n",
      "[0.043397]\n",
      "[0.0403054]\n",
      "[0.03754994]\n",
      "[0.03508003]\n",
      "[0.03285471]\n",
      "[0.0308405]\n",
      "[0.0290097]\n",
      "[0.02733927]\n",
      "[0.02580984]\n",
      "[0.02440501]\n",
      "[0.02311084]\n",
      "[0.02191535]\n",
      "[0.02080825]\n",
      "[0.0197806]\n",
      "[0.01882461]\n",
      "[0.01793349]\n",
      "[0.01710125]\n",
      "[0.01632263]\n",
      "[0.01559294]\n",
      "[0.01490804]\n",
      "[0.01426424]\n",
      "[0.01365821]\n",
      "[0.01308699]\n",
      "[0.01254791]\n",
      "[0.01203856]\n",
      "[0.01155677]\n",
      "[0.01110054]\n",
      "[0.0106681]\n",
      "[0.0102578]\n",
      "[0.00986816]\n",
      "[0.0094978]\n",
      "[0.00914549]\n",
      "[0.00881006]\n",
      "[0.00849048]\n",
      "[0.00818576]\n",
      "[0.00789502]\n",
      "[0.00761742]\n",
      "[0.00735219]\n",
      "[0.00709864]\n",
      "[0.00685609]\n",
      "[0.00662395]\n",
      "[0.00640163]\n",
      "[0.00618862]\n",
      "[0.00598442]\n",
      "[0.00578856]\n",
      "[0.00560061]\n",
      "[0.00542018]\n",
      "[0.00524688]\n",
      "[0.00508037]\n",
      "[0.0049203]\n",
      "[0.00476637]\n",
      "[0.00461829]\n",
      "[0.00447577]\n",
      "[0.00433856]\n",
      "[0.00420642]\n",
      "[0.00407911]\n",
      "[0.00395642]\n",
      "[0.00383814]\n",
      "[0.00372407]\n",
      "[0.00361404]\n",
      "[0.00350786]\n",
      "[0.00340538]\n",
      "[0.00330644]\n",
      "[0.00321088]\n",
      "[0.00311858]\n",
      "[0.00302939]\n",
      "[0.00294319]\n",
      "[0.00285985]\n",
      "[0.00277927]\n",
      "[0.00270134]\n",
      "[0.00262594]\n",
      "[0.00255299]\n",
      "[0.00248238]\n",
      "[0.00241403]\n",
      "[0.00234785]\n",
      "[0.00228377]\n",
      "[0.00222169]\n",
      "[0.00216155]\n",
      "[0.00210328]\n",
      "[0.00204681]\n",
      "[0.00199206]\n",
      "[0.00193899]\n",
      "[0.00188753]\n",
      "[0.95821909]\n",
      "[0.46386431]\n",
      "[0.29940279]\n",
      "[0.21740686]\n",
      "[0.16839119]\n",
      "[0.13586083]\n",
      "[0.11274664]\n",
      "[0.09551417]\n",
      "[0.08219992]\n",
      "[0.07162589]\n",
      "[0.06304251]\n",
      "[0.05595013]\n",
      "[0.05000289]\n",
      "[0.04495382]\n",
      "[0.04062183]\n",
      "[0.03687117]\n",
      "[0.03359805]\n",
      "[0.0307218]\n",
      "[0.02817875]\n",
      "[0.025918]\n",
      "[0.02389837]\n",
      "[0.0220862]\n",
      "[0.02045371]\n",
      "[0.01897775]\n",
      "[0.01763892]\n",
      "[0.01642082]\n",
      "[0.01530949]\n",
      "[0.01429297]\n",
      "[0.01336099]\n",
      "[0.01250465]\n",
      "[0.01171621]\n",
      "[0.01098892]\n",
      "[0.01031685]\n",
      "[0.00969479]\n",
      "[0.00911812]\n",
      "[0.00858276]\n",
      "[0.00808507]\n",
      "[0.00762181]\n",
      "[0.00719008]\n",
      "[0.00678726]\n",
      "[0.00641101]\n",
      "[0.00605923]\n",
      "[0.00572999]\n",
      "[0.00542158]\n",
      "[0.00513242]\n",
      "[0.00486108]\n",
      "[0.00460627]\n",
      "[0.00436679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00414155]\n",
      "[0.00392957]\n",
      "[0.00372994]\n",
      "[0.0035418]\n",
      "[0.0033644]\n",
      "[0.00319703]\n",
      "[0.00303902]\n",
      "[0.00288978]\n",
      "[0.00274874]\n",
      "[0.00261539]\n",
      "[0.00248925]\n",
      "[0.00236988]\n",
      "[0.00225685]\n",
      "[0.0021498]\n",
      "[0.00204835]\n",
      "[0.00195219]\n",
      "[0.00186099]\n",
      "[0.00177447]\n",
      "[0.00169237]\n",
      "[0.00161442]\n",
      "[0.0015404]\n",
      "[0.00147008]\n",
      "[0.00140325]\n",
      "[0.00133973]\n",
      "[0.00127933]\n",
      "[0.00122189]\n",
      "[0.00116723]\n",
      "[0.00111522]\n",
      "[0.00106571]\n",
      "[0.00101857]\n",
      "[0.00097368]\n",
      "[0.00093091]\n",
      "[0.00089017]\n",
      "[0.00085133]\n",
      "[0.00081431]\n",
      "[0.00077901]\n",
      "[0.00074535]\n",
      "[0.00071324]\n",
      "[0.00068261]\n",
      "[0.00065338]\n",
      "[0.00062548]\n",
      "[0.00059885]\n",
      "[0.00057342]\n",
      "[0.00054914]\n",
      "[0.00052595]\n",
      "[0.0005038]\n",
      "[0.00048263]\n",
      "[0.00046241]\n",
      "[0.00044308]\n",
      "[0.0004246]\n",
      "[0.00040694]\n",
      "[0.96910384]\n",
      "[0.4724686]\n",
      "[0.3071244]\n",
      "[0.22459921]\n",
      "[0.17519868]\n",
      "[0.14235811]\n",
      "[0.11897839]\n",
      "[0.10150998]\n",
      "[0.08798099]\n",
      "[0.0772083]\n",
      "[0.06843905]\n",
      "[0.06117134]\n",
      "[0.05505776]\n",
      "[0.04985015]\n",
      "[0.04536657]\n",
      "[0.04147055]\n",
      "[0.03805779]\n",
      "[0.03504714]\n",
      "[0.03237458]\n",
      "[0.02998889]\n",
      "[0.02784862]\n",
      "[0.02591988]\n",
      "[0.02417466]\n",
      "[0.02258966]\n",
      "[0.02114528]\n",
      "[0.01982498]\n",
      "[0.01861466]\n",
      "[0.01750223]\n",
      "[0.0164773]\n",
      "[0.01553086]\n",
      "[0.01465506]\n",
      "[0.01384306]\n",
      "[0.01308882]\n",
      "[0.01238706]\n",
      "[0.01173307]\n",
      "[0.01112269]\n",
      "[0.01055221]\n",
      "[0.0100183]\n",
      "[0.009518]\n",
      "[0.00904863]\n",
      "[0.00860779]\n",
      "[0.0081933]\n",
      "[0.00780319]\n",
      "[0.00743568]\n",
      "[0.00708914]\n",
      "[0.00676209]\n",
      "[0.00645318]\n",
      "[0.00616116]\n",
      "[0.00588492]\n",
      "[0.0056234]\n",
      "[0.00537566]\n",
      "[0.00514081]\n",
      "[0.00491803]\n",
      "[0.00470659]\n",
      "[0.00450578]\n",
      "[0.00431496]\n",
      "[0.00413355]\n",
      "[0.00396098]\n",
      "[0.00379674]\n",
      "[0.00364036]\n",
      "[0.00349139]\n",
      "[0.00334942]\n",
      "[0.00321405]\n",
      "[0.00308494]\n",
      "[0.00296173]\n",
      "[0.00284412]\n",
      "[0.0027318]\n",
      "[0.00262451]\n",
      "[0.00252197]\n",
      "[0.00242395]\n",
      "[0.00233022]\n",
      "[0.00224055]\n",
      "[0.00215475]\n",
      "[0.00207263]\n",
      "[0.00199399]\n",
      "[0.00191869]\n",
      "[0.00184654]\n",
      "[0.00177741]\n",
      "[0.00171115]\n",
      "[0.00164762]\n",
      "[0.0015867]\n",
      "[0.00152827]\n",
      "[0.0014722]\n",
      "[0.0014184]\n",
      "[0.00136676]\n",
      "[0.00131718]\n",
      "[0.00126957]\n",
      "[0.00122385]\n",
      "[0.00117992]\n",
      "[0.00113771]\n",
      "[0.00109715]\n",
      "[0.00105816]\n",
      "[0.00102068]\n",
      "[0.00098464]\n",
      "[0.00094998]\n",
      "[0.00091664]\n",
      "[0.00088457]\n",
      "[0.00085371]\n",
      "[0.00082401]\n",
      "[0.95590624]\n",
      "[0.46974326]\n",
      "[0.30778295]\n",
      "[0.2268721]\n",
      "[0.17838007]\n",
      "[0.14609668]\n",
      "[0.12307471]\n",
      "[0.10584056]\n",
      "[0.09246447]\n",
      "[0.08178857]\n",
      "[0.07307607]\n",
      "[0.06583577]\n",
      "[0.0597276]\n",
      "[0.05450868]\n",
      "[0.05000088]\n",
      "[0.04607064]\n",
      "[0.04261579]\n",
      "[0.0395569]\n",
      "[0.03683124]\n",
      "[0.03438866]\n",
      "[0.03218853]\n",
      "[0.03019764]\n",
      "[0.02838855]\n",
      "[0.02673838]\n",
      "[0.02522792]\n",
      "[0.02384094]\n",
      "[0.02256359]\n",
      "[0.02138402]\n",
      "[0.02029199]\n",
      "[0.01927865]\n",
      "[0.01833629]\n",
      "[0.01745816]\n",
      "[0.01663833]\n",
      "[0.01587157]\n",
      "[0.01515326]\n",
      "[0.01447928]\n",
      "[0.01384596]\n",
      "[0.01325001]\n",
      "[0.01268851]\n",
      "[0.01215879]\n",
      "[0.01165848]\n",
      "[0.0111854]\n",
      "[0.01073761]\n",
      "[0.01031333]\n",
      "[0.00991093]\n",
      "[0.00952893]\n",
      "[0.00916599]\n",
      "[0.00882087]\n",
      "[0.00849242]\n",
      "[0.00817962]\n",
      "[0.00788149]\n",
      "[0.00759714]\n",
      "[0.00732576]\n",
      "[0.0070666]\n",
      "[0.00681894]\n",
      "[0.00658213]\n",
      "[0.00635558]\n",
      "[0.00613871]\n",
      "[0.00593101]\n",
      "[0.00573198]\n",
      "[0.00554116]\n",
      "[0.00535814]\n",
      "[0.00518252]\n",
      "[0.00501391]\n",
      "[0.00485198]\n",
      "[0.00469638]\n",
      "[0.00454682]\n",
      "[0.004403]\n",
      "[0.00426465]\n",
      "[0.00413152]\n",
      "[0.00400336]\n",
      "[0.00387995]\n",
      "[0.00376107]\n",
      "[0.00364651]\n",
      "[0.00353609]\n",
      "[0.00342962]\n",
      "[0.00332693]\n",
      "[0.00322787]\n",
      "[0.00313226]\n",
      "[0.00303998]\n",
      "[0.00295088]\n",
      "[0.00286482]\n",
      "[0.00278169]\n",
      "[0.00270136]\n",
      "[0.00262372]\n",
      "[0.00254867]\n",
      "[0.0024761]\n",
      "[0.00240591]\n",
      "[0.00233802]\n",
      "[0.00227233]\n",
      "[0.00220875]\n",
      "[0.00214722]\n",
      "[0.00208764]\n",
      "[0.00202996]\n",
      "[0.00197409]\n",
      "[0.00191997]\n",
      "[0.00186753]\n",
      "[0.00181673]\n",
      "[0.00176748]\n",
      "[0.92322735]\n",
      "[0.43397358]\n",
      "[0.27199233]\n",
      "[0.19177966]\n",
      "[0.14423715]\n",
      "[0.11300054]\n",
      "[0.09105805]\n",
      "[0.07490504]\n",
      "[0.0625955]\n",
      "[0.05296272]\n",
      "[0.04526497]\n",
      "[0.03900841]\n",
      "[0.03385172]\n",
      "[0.02955158]\n",
      "[0.02592998]\n",
      "[0.02285378]\n",
      "[0.02022151]\n",
      "[0.01795456]\n",
      "[0.0159911]\n",
      "[0.01428192]\n",
      "[0.01278739]\n",
      "[0.01147527]\n",
      "[0.01031912]\n",
      "[0.00929702]\n",
      "[0.00839073]\n",
      "[0.00758492]\n",
      "[0.00686665]\n",
      "[0.00622494]\n",
      "[0.00565041]\n",
      "[0.00513501]\n",
      "[0.00467181]\n",
      "[0.00425483]\n",
      "[0.00387885]\n",
      "[0.00353934]\n",
      "[0.00323235]\n",
      "[0.00295439]\n",
      "[0.00270242]\n",
      "[0.00247375]\n",
      "[0.002266]\n",
      "[0.00207706]\n",
      "[0.00190506]\n",
      "[0.00174835]\n",
      "[0.00160544]\n",
      "[0.00147501]\n",
      "[0.00135587]\n",
      "[0.00124698]\n",
      "[0.00114737]\n",
      "[0.0010562]\n",
      "[0.00097269]\n",
      "[0.00089616]\n",
      "[0.00082598]\n",
      "[0.00076159]\n",
      "[0.00070248]\n",
      "[0.00064819]\n",
      "[0.0005983]\n",
      "[0.00055243]\n",
      "[0.00051024]\n",
      "[0.00047142]\n",
      "[0.00043568]\n",
      "[0.00040276]\n",
      "[0.00037244]\n",
      "[0.00034449]\n",
      "[0.00031872]\n",
      "[0.00029496]\n",
      "[0.00027303]\n",
      "[0.00025279]\n",
      "[0.00023411]\n",
      "[0.00021685]\n",
      "[0.00020092]\n",
      "[0.00018619]\n",
      "[0.00017257]\n",
      "[0.00015999]\n",
      "[0.00014835]\n",
      "[0.00013758]\n",
      "[0.00012762]\n",
      "[0.0001184]\n",
      "[0.00010986]\n",
      "[0.00010196]\n",
      "[9.46416003e-05]\n",
      "[8.78625496e-05]\n",
      "[8.15818203e-05]\n",
      "[7.57616073e-05]\n",
      "[7.03670852e-05]\n",
      "[6.53661626e-05]\n",
      "[6.07292576e-05]\n",
      "[5.64290928e-05]\n",
      "[5.24405079e-05]\n",
      "[4.8740288e-05]\n",
      "[4.53070076e-05]\n",
      "[4.2120886e-05]\n",
      "[3.91636569e-05]\n",
      "[3.64184471e-05]\n",
      "[3.38696668e-05]\n",
      "[3.15029077e-05]\n",
      "[2.93048506e-05]\n",
      "[2.72631797e-05]\n",
      "[2.53665045e-05]\n",
      "[2.36042881e-05]\n",
      "[2.19667805e-05]\n",
      "[0.89853564]\n",
      "[0.41191998]\n",
      "[0.25178459]\n",
      "[0.17314021]\n",
      "[0.12699759]\n",
      "[0.09703352]\n",
      "[0.0762575]\n",
      "[0.0611784]\n",
      "[0.0498601]\n",
      "[0.04114369]\n",
      "[0.03429399]\n",
      "[0.02882286]\n",
      "[0.02439397]\n",
      "[0.02076851]\n",
      "[0.01777255]\n",
      "[0.01527666]\n",
      "[0.01318278]\n",
      "[0.0114154]\n",
      "[0.00991557]\n",
      "[0.00863672]\n",
      "[0.00754166]\n",
      "[0.00660041]\n",
      "[0.0057886]\n",
      "[0.00508625]\n",
      "[0.00447689]\n",
      "[0.00394685]\n",
      "[0.00348472]\n",
      "[0.00308092]\n",
      "[0.0027274]\n",
      "[0.00241731]\n",
      "[0.00214487]\n",
      "[0.00190511]\n",
      "[0.0016938]\n",
      "[0.00150732]\n",
      "[0.00134253]\n",
      "[0.00119673]\n",
      "[0.00106759]\n",
      "[0.00095308]\n",
      "[0.00085145]\n",
      "[0.00076115]\n",
      "[0.00068085]\n",
      "[0.00060939]\n",
      "[0.00054574]\n",
      "[0.000489]\n",
      "[0.00043838]\n",
      "[0.0003932]\n",
      "[0.00035285]\n",
      "[0.00031677]\n",
      "[0.00028451]\n",
      "[0.00025564]\n",
      "[0.0002298]\n",
      "[0.00020664]\n",
      "[0.00018589]\n",
      "[0.00016728]\n",
      "[0.00015058]\n",
      "[0.0001356]\n",
      "[0.00012215]\n",
      "[0.00011006]\n",
      "[9.92021102e-05]\n",
      "[8.943947e-05]\n",
      "[8.06599925e-05]\n",
      "[7.27618727e-05]\n",
      "[6.56542059e-05]\n",
      "[5.92557738e-05]\n",
      "[5.34939708e-05]\n",
      "[4.83038561e-05]\n",
      "[4.36273144e-05]\n",
      "[3.94123123e-05]\n",
      "[3.56122392e-05]\n",
      "[3.21853235e-05]\n",
      "[2.90941125e-05]\n",
      "[2.63050125e-05]\n",
      "[2.37878775e-05]\n",
      "[2.15156454e-05]\n",
      "[1.94640127e-05]\n",
      "[1.76111451e-05]\n",
      "[1.59374197e-05]\n",
      "[1.44251944e-05]\n",
      "[1.30586034e-05]\n",
      "[1.18233728e-05]\n",
      "[1.07066571e-05]\n",
      "[9.69689307e-06]\n",
      "[8.78366799e-06]\n",
      "[7.95760289e-06]\n",
      "[7.21024743e-06]\n",
      "[6.53398582e-06]\n",
      "[5.92195266e-06]\n",
      "[5.36795736e-06]\n",
      "[4.86641641e-06]\n",
      "[4.41229265e-06]\n",
      "[4.00104073e-06]\n",
      "[3.62855811e-06]\n",
      "[3.29114113e-06]\n",
      "[2.98544549e-06]\n",
      "[2.70845073e-06]\n",
      "[2.45742831e-06]\n",
      "[2.2299129e-06]\n",
      "[2.02367657e-06]\n",
      "[1.83670552e-06]\n",
      "[0.97342766]\n",
      "[0.481406]\n",
      "[0.31743737]\n",
      "[0.23548168]\n",
      "[0.18633092]\n",
      "[0.15358241]\n",
      "[0.13020645]\n",
      "[0.11268818]\n",
      "[0.0990749]\n",
      "[0.088195]\n",
      "[0.07930291]\n",
      "[0.07190157]\n",
      "[0.06564687]\n",
      "[0.06029304]\n",
      "[0.05565982]\n",
      "[0.05161202]\n",
      "[0.04804628]\n",
      "[0.04488218]\n",
      "[0.04205626]\n",
      "[0.03951774]\n",
      "[0.0372255]\n",
      "[0.03514593]\n",
      "[0.03325123]\n",
      "[0.03151825]\n",
      "[0.02992755]\n",
      "[0.02846267]\n",
      "[0.02710959]\n",
      "[0.02585631]\n",
      "[0.02469246]\n",
      "[0.02360907]\n",
      "[0.02259833]\n",
      "[0.02165339]\n",
      "[0.02076824]\n",
      "[0.01993758]\n",
      "[0.01915672]\n",
      "[0.01842148]\n",
      "[0.01772814]\n",
      "[0.01707336]\n",
      "[0.01645417]\n",
      "[0.01586786]\n",
      "[0.01531201]\n",
      "[0.01478443]\n",
      "[0.01428313]\n",
      "[0.01380629]\n",
      "[0.01335226]\n",
      "[0.01291955]\n",
      "[0.01250677]\n",
      "[0.01211266]\n",
      "[0.01173606]\n",
      "[0.01137592]\n",
      "[0.01103123]\n",
      "[0.01070111]\n",
      "[0.0103847]\n",
      "[0.01008124]\n",
      "[0.00979]\n",
      "[0.00951032]\n",
      "[0.00924158]\n",
      "[0.0089832]\n",
      "[0.00873463]\n",
      "[0.00849539]\n",
      "[0.00826499]\n",
      "[0.00804301]\n",
      "[0.00782902]\n",
      "[0.00762265]\n",
      "[0.00742353]\n",
      "[0.00723132]\n",
      "[0.0070457]\n",
      "[0.00686638]\n",
      "[0.00669307]\n",
      "[0.00652551]\n",
      "[0.00636344]\n",
      "[0.00620663]\n",
      "[0.00605485]\n",
      "[0.00590789]\n",
      "[0.00576555]\n",
      "[0.00562763]\n",
      "[0.00549397]\n",
      "[0.00536439]\n",
      "[0.00523873]\n",
      "[0.00511683]\n",
      "[0.00499854]\n",
      "[0.00488374]\n",
      "[0.00477228]\n",
      "[0.00466404]\n",
      "[0.00455891]\n",
      "[0.00445676]\n",
      "[0.00435749]\n",
      "[0.00426099]\n",
      "[0.00416717]\n",
      "[0.00407593]\n",
      "[0.00398717]\n",
      "[0.00390083]\n",
      "[0.0038168]\n",
      "[0.00373501]\n",
      "[0.00365539]\n",
      "[0.00357787]\n",
      "[0.00350237]\n",
      "[0.00342882]\n",
      "[0.00335717]\n",
      "[0.9895512]\n",
      "[0.49306855]\n",
      "[0.32757826]\n",
      "[0.24483605]\n",
      "[0.19519306]\n",
      "[0.16209968]\n",
      "[0.13846321]\n",
      "[0.12073731]\n",
      "[0.10695177]\n",
      "[0.0959245]\n",
      "[0.08690322]\n",
      "[0.07938644]\n",
      "[0.07302697]\n",
      "[0.0675768]\n",
      "[0.06285407]\n",
      "[0.05872239]\n",
      "[0.05507745]\n",
      "[0.05183812]\n",
      "[0.04894036]\n",
      "[0.04633293]\n",
      "[0.04397436]\n",
      "[0.0418307]\n",
      "[0.03987393]\n",
      "[0.03808068]\n",
      "[0.03643132]\n",
      "[0.03490926]\n",
      "[0.03350034]\n",
      "[0.03219245]\n",
      "[0.03097512]\n",
      "[0.02983931]\n",
      "[0.02877713]\n",
      "[0.02778166]\n",
      "[0.02684684]\n",
      "[0.02596733]\n",
      "[0.02513837]\n",
      "[0.02435576]\n",
      "[0.02361574]\n",
      "[0.02291494]\n",
      "[0.02225034]\n",
      "[0.02161924]\n",
      "[0.02101917]\n",
      "[0.02044792]\n",
      "[0.01990348]\n",
      "[0.01938402]\n",
      "[0.01888787]\n",
      "[0.01841352]\n",
      "[0.01795956]\n",
      "[0.01752473]\n",
      "[0.01710785]\n",
      "[0.01670785]\n",
      "[0.01632373]\n",
      "[0.01595458]\n",
      "[0.01559954]\n",
      "[0.01525784]\n",
      "[0.01492874]\n",
      "[0.01461157]\n",
      "[0.0143057]\n",
      "[0.01401054]\n",
      "[0.01372555]\n",
      "[0.01345023]\n",
      "[0.01318409]\n",
      "[0.01292669]\n",
      "[0.01267761]\n",
      "[0.01243647]\n",
      "[0.01220289]\n",
      "[0.01197653]\n",
      "[0.01175708]\n",
      "[0.01154421]\n",
      "[0.01133765]\n",
      "[0.01113713]\n",
      "[0.01094238]\n",
      "[0.01075318]\n",
      "[0.01056928]\n",
      "[0.01039048]\n",
      "[0.01021657]\n",
      "[0.01004736]\n",
      "[0.00988266]\n",
      "[0.0097223]\n",
      "[0.00956611]\n",
      "[0.00941394]\n",
      "[0.00926564]\n",
      "[0.00912107]\n",
      "[0.00898009]\n",
      "[0.00884257]\n",
      "[0.00870839]\n",
      "[0.00857743]\n",
      "[0.00844959]\n",
      "[0.00832475]\n",
      "[0.00820281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00808368]\n",
      "[0.00796727]\n",
      "[0.00785348]\n",
      "[0.00774223]\n",
      "[0.00763344]\n",
      "[0.00752702]\n",
      "[0.00742292]\n",
      "[0.00732105]\n",
      "[0.00722134]\n",
      "[0.00712374]\n",
      "[0.96483234]\n",
      "[0.46975939]\n",
      "[0.30495645]\n",
      "[0.22271666]\n",
      "[0.17349873]\n",
      "[0.14078899]\n",
      "[0.11751018]\n",
      "[0.10012377]\n",
      "[0.08666391]\n",
      "[0.07595116]\n",
      "[0.06723499]\n",
      "[0.06001508]\n",
      "[0.05394509]\n",
      "[0.04877764]\n",
      "[0.04433138]\n",
      "[0.04047027]\n",
      "[0.03709034]\n",
      "[0.03411071]\n",
      "[0.03146758]\n",
      "[0.02910989]\n",
      "[0.02699634]\n",
      "[0.02509315]\n",
      "[0.02337241]\n",
      "[0.02181091]\n",
      "[0.02038913]\n",
      "[0.01909057]\n",
      "[0.0179012]\n",
      "[0.01680898]\n",
      "[0.01580357]\n",
      "[0.01487598]\n",
      "[0.01401841]\n",
      "[0.01322403]\n",
      "[0.01248687]\n",
      "[0.01180164]\n",
      "[0.01116366]\n",
      "[0.01056881]\n",
      "[0.01001337]\n",
      "[0.00949406]\n",
      "[0.00900792]\n",
      "[0.0085523]\n",
      "[0.0081248]\n",
      "[0.00772326]\n",
      "[0.00734573]\n",
      "[0.00699044]\n",
      "[0.00665577]\n",
      "[0.00634025]\n",
      "[0.00604255]\n",
      "[0.00576143]\n",
      "[0.00549578]\n",
      "[0.00524456]\n",
      "[0.00500683]\n",
      "[0.00478171]\n",
      "[0.0045684]\n",
      "[0.00436616]\n",
      "[0.00417431]\n",
      "[0.0039922]\n",
      "[0.00381926]\n",
      "[0.00365494]\n",
      "[0.00349872]\n",
      "[0.00335015]\n",
      "[0.00320877]\n",
      "[0.00307419]\n",
      "[0.00294602]\n",
      "[0.0028239]\n",
      "[0.00270751]\n",
      "[0.00259653]\n",
      "[0.00249067]\n",
      "[0.00238965]\n",
      "[0.00229324]\n",
      "[0.00220117]\n",
      "[0.00211323]\n",
      "[0.00202921]\n",
      "[0.0019489]\n",
      "[0.00187212]\n",
      "[0.0017987]\n",
      "[0.00172846]\n",
      "[0.00166125]\n",
      "[0.00159693]\n",
      "[0.00153535]\n",
      "[0.00147638]\n",
      "[0.00141989]\n",
      "[0.00136578]\n",
      "[0.00131392]\n",
      "[0.00126422]\n",
      "[0.00121657]\n",
      "[0.00117088]\n",
      "[0.00112705]\n",
      "[0.00108501]\n",
      "[0.00104467]\n",
      "[0.00100596]\n",
      "[0.0009688]\n",
      "[0.00093313]\n",
      "[0.00089888]\n",
      "[0.00086598]\n",
      "[0.00083439]\n",
      "[0.00080403]\n",
      "[0.00077487]\n",
      "[0.00074684]\n",
      "[0.0007199]\n",
      "[0.99473817]\n",
      "[0.4956389]\n",
      "[0.3292765]\n",
      "[0.24609829]\n",
      "[0.19619376]\n",
      "[0.16292606]\n",
      "[0.13916511]\n",
      "[0.12134587]\n",
      "[0.10748778]\n",
      "[0.09640248]\n",
      "[0.08733375]\n",
      "[0.07977745]\n",
      "[0.07338455]\n",
      "[0.06790575]\n",
      "[0.06315823]\n",
      "[0.05900487]\n",
      "[0.05534081]\n",
      "[0.0520845]\n",
      "[0.04917156]\n",
      "[0.04655049]\n",
      "[0.04417958]\n",
      "[0.04202471]\n",
      "[0.04005772]\n",
      "[0.03825511]\n",
      "[0.03659715]\n",
      "[0.03506715]\n",
      "[0.0336509]\n",
      "[0.0323362]\n",
      "[0.03111255]\n",
      "[0.02997085]\n",
      "[0.02890315]\n",
      "[0.02790252]\n",
      "[0.02696287]\n",
      "[0.02607881]\n",
      "[0.02524557]\n",
      "[0.02445893]\n",
      "[0.02371509]\n",
      "[0.02301068]\n",
      "[0.02234267]\n",
      "[0.02170832]\n",
      "[0.02110518]\n",
      "[0.020531]\n",
      "[0.01998378]\n",
      "[0.01946167]\n",
      "[0.01896299]\n",
      "[0.01848622]\n",
      "[0.01802996]\n",
      "[0.01759292]\n",
      "[0.01717393]\n",
      "[0.0167719]\n",
      "[0.01638584]\n",
      "[0.01601482]\n",
      "[0.015658]\n",
      "[0.01531458]\n",
      "[0.01498382]\n",
      "[0.01466506]\n",
      "[0.01435766]\n",
      "[0.01406103]\n",
      "[0.01377462]\n",
      "[0.01349793]\n",
      "[0.01323047]\n",
      "[0.01297179]\n",
      "[0.01272148]\n",
      "[0.01247914]\n",
      "[0.01224441]\n",
      "[0.01201694]\n",
      "[0.01179641]\n",
      "[0.0115825]\n",
      "[0.01137493]\n",
      "[0.01117343]\n",
      "[0.01097773]\n",
      "[0.01078761]\n",
      "[0.01060282]\n",
      "[0.01042315]\n",
      "[0.0102484]\n",
      "[0.01007837]\n",
      "[0.00991288]\n",
      "[0.00975175]\n",
      "[0.00959482]\n",
      "[0.00944192]\n",
      "[0.00929292]\n",
      "[0.00914765]\n",
      "[0.009006]\n",
      "[0.00886783]\n",
      "[0.00873302]\n",
      "[0.00860145]\n",
      "[0.008473]\n",
      "[0.00834758]\n",
      "[0.00822507]\n",
      "[0.00810539]\n",
      "[0.00798843]\n",
      "[0.00787412]\n",
      "[0.00776235]\n",
      "[0.00765306]\n",
      "[0.00754616]\n",
      "[0.00744157]\n",
      "[0.00733924]\n",
      "[0.00723908]\n",
      "[0.00714103]\n",
      "[0.95943891]\n",
      "[0.4658264]\n",
      "[0.30155714]\n",
      "[0.21961785]\n",
      "[0.17060604]\n",
      "[0.13805429]\n",
      "[0.11490526]\n",
      "[0.09763032]\n",
      "[0.08426922]\n",
      "[0.07364584]\n",
      "[0.06501182]\n",
      "[0.05786827]\n",
      "[0.05186987]\n",
      "[0.04676999]\n",
      "[0.04238779]\n",
      "[0.0385877]\n",
      "[0.03526604]\n",
      "[0.03234222]\n",
      "[0.02975264]\n",
      "[0.02744643]\n",
      "[0.02538244]\n",
      "[0.02352701]\n",
      "[0.02185236]\n",
      "[0.02033535]\n",
      "[0.01895657]\n",
      "[0.01769959]\n",
      "[0.01655044]\n",
      "[0.01549716]\n",
      "[0.01452944]\n",
      "[0.01363837]\n",
      "[0.01281618]\n",
      "[0.01205611]\n",
      "[0.0113522]\n",
      "[0.01069921]\n",
      "[0.01009251]\n",
      "[0.009528]\n",
      "[0.00900201]\n",
      "[0.00851127]\n",
      "[0.00805286]\n",
      "[0.00762415]\n",
      "[0.00722278]\n",
      "[0.00684661]\n",
      "[0.00649371]\n",
      "[0.00616234]\n",
      "[0.0058509]\n",
      "[0.00555794]\n",
      "[0.00528215]\n",
      "[0.00502232]\n",
      "[0.00477734]\n",
      "[0.0045462]\n",
      "[0.00432798]\n",
      "[0.00412182]\n",
      "[0.00392693]\n",
      "[0.00374259]\n",
      "[0.00356813]\n",
      "[0.00340292]\n",
      "[0.0032464]\n",
      "[0.00309803]\n",
      "[0.00295732]\n",
      "[0.00282381]\n",
      "[0.00269708]\n",
      "[0.00257673]\n",
      "[0.00246239]\n",
      "[0.00235371]\n",
      "[0.00225039]\n",
      "[0.0021521]\n",
      "[0.00205859]\n",
      "[0.00196957]\n",
      "[0.00188481]\n",
      "[0.00180408]\n",
      "[0.00172716]\n",
      "[0.00165385]\n",
      "[0.00158395]\n",
      "[0.00151729]\n",
      "[0.00145371]\n",
      "[0.00139303]\n",
      "[0.00133512]\n",
      "[0.00127983]\n",
      "[0.00122704]\n",
      "[0.00117661]\n",
      "[0.00112843]\n",
      "[0.00108238]\n",
      "[0.00103837]\n",
      "[0.0009963]\n",
      "[0.00095606]\n",
      "[0.00091758]\n",
      "[0.00088077]\n",
      "[0.00084554]\n",
      "[0.00081183]\n",
      "[0.00077956]\n",
      "[0.00074866]\n",
      "[0.00071908]\n",
      "[0.00069074]\n",
      "[0.0006636]\n",
      "[0.0006376]\n",
      "[0.00061269]\n",
      "[0.00058881]\n",
      "[0.00056592]\n",
      "[0.00054398]\n",
      "[0.97532703]\n",
      "[0.48011215]\n",
      "[0.31511848]\n",
      "[0.2326792]\n",
      "[0.18326097]\n",
      "[0.15035268]\n",
      "[0.12687814]\n",
      "[0.10929928]\n",
      "[0.09565049]\n",
      "[0.08475243]\n",
      "[0.0758546]\n",
      "[0.06845667]\n",
      "[0.06221228]\n",
      "[0.05687401]\n",
      "[0.05226044]\n",
      "[0.0482355]\n",
      "[0.04469513]\n",
      "[0.04155843]\n",
      "[0.03876149]\n",
      "[0.03625321]\n",
      "[0.03399223]\n",
      "[0.03194469]\n",
      "[0.03008264]\n",
      "[0.02838278]\n",
      "[0.02682555]\n",
      "[0.02539439]\n",
      "[0.02407519]\n",
      "[0.02285588]\n",
      "[0.02172603]\n",
      "[0.02067662]\n",
      "[0.01969979]\n",
      "[0.01878865]\n",
      "[0.01793718]\n",
      "[0.01714003]\n",
      "[0.01639249]\n",
      "[0.01569036]\n",
      "[0.0150299]\n",
      "[0.01440777]\n",
      "[0.01382096]\n",
      "[0.01326677]\n",
      "[0.01274277]\n",
      "[0.01224675]\n",
      "[0.01177671]\n",
      "[0.01133084]\n",
      "[0.01090749]\n",
      "[0.01050514]\n",
      "[0.01012242]\n",
      "[0.00975806]\n",
      "[0.00941089]\n",
      "[0.00907987]\n",
      "[0.00876399]\n",
      "[0.00846235]\n",
      "[0.00817412]\n",
      "[0.00789851]\n",
      "[0.00763482]\n",
      "[0.00738237]\n",
      "[0.00714055]\n",
      "[0.00690877]\n",
      "[0.00668651]\n",
      "[0.00647325]\n",
      "[0.00626854]\n",
      "[0.00607193]\n",
      "[0.00588302]\n",
      "[0.00570143]\n",
      "[0.00552678]\n",
      "[0.00535876]\n",
      "[0.00519704]\n",
      "[0.00504132]\n",
      "[0.00489133]\n",
      "[0.00474679]\n",
      "[0.00460747]\n",
      "[0.00447312]\n",
      "[0.00434353]\n",
      "[0.00421848]\n",
      "[0.00409778]\n",
      "[0.00398125]\n",
      "[0.00386869]\n",
      "[0.00375996]\n",
      "[0.00365488]\n",
      "[0.0035533]\n",
      "[0.00345509]\n",
      "[0.00336011]\n",
      "[0.00326822]\n",
      "[0.00317931]\n",
      "[0.00309325]\n",
      "[0.00300994]\n",
      "[0.00292928]\n",
      "[0.00285114]\n",
      "[0.00277546]\n",
      "[0.00270212]\n",
      "[0.00263104]\n",
      "[0.00256215]\n",
      "[0.00249535]\n",
      "[0.00243057]\n",
      "[0.00236775]\n",
      "[0.0023068]\n",
      "[0.00224767]\n",
      "[0.00219028]\n",
      "[0.00213458]\n",
      "[0.99362317]\n",
      "[0.49480958]\n",
      "[0.32854376]\n",
      "[0.24541486]\n",
      "[0.19554073]\n",
      "[0.16229396]\n",
      "[0.13854854]\n",
      "[0.12074145]\n",
      "[0.10689324]\n",
      "[0.09581624]\n",
      "[0.08675466]\n",
      "[0.07920465]\n",
      "[0.07281736]\n",
      "[0.06734365]\n",
      "[0.06260079]\n",
      "[0.05845174]\n",
      "[0.05479171]\n",
      "[0.0515392]\n",
      "[0.04862985]\n",
      "[0.0460122]\n",
      "[0.04364455]\n",
      "[0.04149283]\n",
      "[0.03952886]\n",
      "[0.03772917]\n",
      "[0.03607405]\n",
      "[0.0345468]\n",
      "[0.03313324]\n",
      "[0.03182116]\n",
      "[0.03060007]\n",
      "[0.02946087]\n",
      "[0.02839563]\n",
      "[0.02739741]\n",
      "[0.02646013]\n",
      "[0.0255784]\n",
      "[0.02474746]\n",
      "[0.02396307]\n",
      "[0.02322147]\n",
      "[0.02251926]\n",
      "[0.02185343]\n",
      "[0.02122123]\n",
      "[0.02062021]\n",
      "[0.02004814]\n",
      "[0.01950299]\n",
      "[0.01898294]\n",
      "[0.0184863]\n",
      "[0.01801155]\n",
      "[0.01755729]\n",
      "[0.01712223]\n",
      "[0.01670521]\n",
      "[0.01630513]\n",
      "[0.01592101]\n",
      "[0.01555191]\n",
      "[0.01519699]\n",
      "[0.01485546]\n",
      "[0.01452659]\n",
      "[0.01420969]\n",
      "[0.01390414]\n",
      "[0.01360935]\n",
      "[0.01332477]\n",
      "[0.01304989]\n",
      "[0.01278423]\n",
      "[0.01252735]\n",
      "[0.01227882]\n",
      "[0.01203826]\n",
      "[0.01180529]\n",
      "[0.01157957]\n",
      "[0.01136078]\n",
      "[0.0111486]\n",
      "[0.01094275]\n",
      "[0.01074296]\n",
      "[0.01054897]\n",
      "[0.01036054]\n",
      "[0.01017743]\n",
      "[0.00999944]\n",
      "[0.00982636]\n",
      "[0.00965799]\n",
      "[0.00949415]\n",
      "[0.00933466]\n",
      "[0.00917936]\n",
      "[0.00902809]\n",
      "[0.0088807]\n",
      "[0.00873705]\n",
      "[0.008597]\n",
      "[0.00846042]\n",
      "[0.0083272]\n",
      "[0.0081972]\n",
      "[0.00807033]\n",
      "[0.00794647]\n",
      "[0.00782552]\n",
      "[0.00770739]\n",
      "[0.00759197]\n",
      "[0.00747919]\n",
      "[0.00736895]\n",
      "[0.00726118]\n",
      "[0.0071558]\n",
      "[0.00705272]\n",
      "[0.00695188]\n",
      "[0.00685322]\n",
      "[0.00675666]\n",
      "[0.97148011]\n",
      "[0.47603982]\n",
      "[0.31102219]\n",
      "[0.22860831]\n",
      "[0.17923439]\n",
      "[0.14637923]\n",
      "[0.12296232]\n",
      "[0.10544341]\n",
      "[0.09185574]\n",
      "[0.08101924]\n",
      "[0.07218298]\n",
      "[0.06484636]\n",
      "[0.05866281]\n",
      "[0.05338479]\n",
      "[0.04883078]\n",
      "[0.04486465]\n",
      "[0.04138231]\n",
      "[0.0383028]\n",
      "[0.03556221]\n",
      "[0.03310943]\n",
      "[0.03090308]\n",
      "[0.02890931]\n",
      "[0.02710017]\n",
      "[0.02545235]\n",
      "[0.02394631]\n",
      "[0.02256548]\n",
      "[0.02129578]\n",
      "[0.02012513]\n",
      "[0.01904312]\n",
      "[0.01804073]\n",
      "[0.01711012]\n",
      "[0.01624442]\n",
      "[0.01543759]\n",
      "[0.01468432]\n",
      "[0.0139799]\n",
      "[0.01332015]\n",
      "[0.01270133]\n",
      "[0.01212011]\n",
      "[0.01157351]\n",
      "[0.01105882]\n",
      "[0.01057364]\n",
      "[0.01011576]\n",
      "[0.00968319]\n",
      "[0.00927414]\n",
      "[0.00888696]\n",
      "[0.00852015]\n",
      "[0.00817234]\n",
      "[0.00784228]\n",
      "[0.00752882]\n",
      "[0.0072309]\n",
      "[0.00694755]\n",
      "[0.00667787]\n",
      "[0.00642103]\n",
      "[0.00617627]\n",
      "[0.00594288]\n",
      "[0.00572019]\n",
      "[0.00550761]\n",
      "[0.00530456]\n",
      "[0.00511052]\n",
      "[0.00492498]\n",
      "[0.00474751]\n",
      "[0.00457766]\n",
      "[0.00441503]\n",
      "[0.00425925]\n",
      "[0.00410998]\n",
      "[0.00396687]\n",
      "[0.00382963]\n",
      "[0.00369796]\n",
      "[0.00357159]\n",
      "[0.00345026]\n",
      "[0.00333373]\n",
      "[0.00322178]\n",
      "[0.00311419]\n",
      "[0.00301076]\n",
      "[0.00291129]\n",
      "[0.00281561]\n",
      "[0.00272354]\n",
      "[0.00263494]\n",
      "[0.00254963]\n",
      "[0.00246748]\n",
      "[0.00238835]\n",
      "[0.00231211]\n",
      "[0.00223864]\n",
      "[0.00216781]\n",
      "[0.00209953]\n",
      "[0.00203367]\n",
      "[0.00197015]\n",
      "[0.00190887]\n",
      "[0.00184973]\n",
      "[0.00179265]\n",
      "[0.00173754]\n",
      "[0.00168433]\n",
      "[0.00163295]\n",
      "[0.00158331]\n",
      "[0.00153536]\n",
      "[0.00148902]\n",
      "[0.00144424]\n",
      "[0.00140096]\n",
      "[0.00135911]\n",
      "[0.98899268]\n",
      "[0.49186624]\n",
      "[0.32616675]\n",
      "[0.24332396]\n",
      "[0.19362383]\n",
      "[0.16049499]\n",
      "[0.13683545]\n",
      "[0.1190942]\n",
      "[0.10529846]\n",
      "[0.09426456]\n",
      "[0.08523927]\n",
      "[0.07772041]\n",
      "[0.07136034]\n",
      "[0.06591073]\n",
      "[0.06118949]\n",
      "[0.05706004]\n",
      "[0.05341793]\n",
      "[0.05018193]\n",
      "[0.04728792]\n",
      "[0.04468459]\n",
      "[0.0423304]\n",
      "[0.04019138]\n",
      "[0.03823946]\n",
      "[0.03645123]\n",
      "[0.03480707]\n",
      "[0.03329032]\n",
      "[0.03188684]\n",
      "[0.03058449]\n",
      "[0.02937279]\n",
      "[0.02824267]\n",
      "[0.02718625]\n",
      "[0.0261966]\n",
      "[0.02526765]\n",
      "[0.02439405]\n",
      "[0.02357104]\n",
      "[0.0227944]\n",
      "[0.02206037]\n",
      "[0.02136559]\n",
      "[0.02070703]\n",
      "[0.02008197]\n",
      "[0.01948796]\n",
      "[0.01892278]\n",
      "[0.01838441]\n",
      "[0.01787102]\n",
      "[0.01738095]\n",
      "[0.01691267]\n",
      "[0.01646478]\n",
      "[0.01603602]\n",
      "[0.0156252]\n",
      "[0.01523125]\n",
      "[0.01485318]\n",
      "[0.01449006]\n",
      "[0.01414105]\n",
      "[0.01380536]\n",
      "[0.01348226]\n",
      "[0.01317108]\n",
      "[0.01287118]\n",
      "[0.01258198]\n",
      "[0.01230294]\n",
      "[0.01203355]\n",
      "[0.01177332]\n",
      "[0.01152182]\n",
      "[0.01127863]\n",
      "[0.01104335]\n",
      "[0.01081562]\n",
      "[0.01059509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01038144]\n",
      "[0.01017437]\n",
      "[0.00997358]\n",
      "[0.00977882]\n",
      "[0.00958981]\n",
      "[0.00940632]\n",
      "[0.00922812]\n",
      "[0.009055]\n",
      "[0.00888674]\n",
      "[0.00872317]\n",
      "[0.00856409]\n",
      "[0.00840932]\n",
      "[0.00825872]\n",
      "[0.00811211]\n",
      "[0.00796934]\n",
      "[0.00783029]\n",
      "[0.0076948]\n",
      "[0.00756276]\n",
      "[0.00743403]\n",
      "[0.00730851]\n",
      "[0.00718608]\n",
      "[0.00706663]\n",
      "[0.00695007]\n",
      "[0.00683629]\n",
      "[0.0067252]\n",
      "[0.00661672]\n",
      "[0.00651076]\n",
      "[0.00640724]\n",
      "[0.00630607]\n",
      "[0.00620719]\n",
      "[0.00611053]\n",
      "[0.00601601]\n",
      "[0.00592356]\n",
      "[0.97782114]\n",
      "[0.47807073]\n",
      "[0.31164749]\n",
      "[0.22855337]\n",
      "[0.17878882]\n",
      "[0.14568735]\n",
      "[0.12210622]\n",
      "[0.10447408]\n",
      "[0.09080689]\n",
      "[0.07991421]\n",
      "[0.07103855]\n",
      "[0.0636749]\n",
      "[0.05747366]\n",
      "[0.05218515]\n",
      "[0.04762626]\n",
      "[0.04365967]\n",
      "[0.0401804]\n",
      "[0.03710679]\n",
      "[0.03437439]\n",
      "[0.03193165]\n",
      "[0.02973684]\n",
      "[0.02775583]\n",
      "[0.02596042]\n",
      "[0.02432714]\n",
      "[0.02283626]\n",
      "[0.02147111]\n",
      "[0.02021747]\n",
      "[0.01906317]\n",
      "[0.01799774]\n",
      "[0.01701208]\n",
      "[0.01609829]\n",
      "[0.01524945]\n",
      "[0.01445949]\n",
      "[0.01372305]\n",
      "[0.0130354]\n",
      "[0.01239232]\n",
      "[0.01179006]\n",
      "[0.01122527]\n",
      "[0.01069495]\n",
      "[0.01019638]\n",
      "[0.00972713]\n",
      "[0.00928501]\n",
      "[0.008868]\n",
      "[0.00847431]\n",
      "[0.00810228]\n",
      "[0.00775041]\n",
      "[0.00741733]\n",
      "[0.00710177]\n",
      "[0.00680259]\n",
      "[0.00651874]\n",
      "[0.00624922]\n",
      "[0.00599316]\n",
      "[0.00574971]\n",
      "[0.00551811]\n",
      "[0.00529766]\n",
      "[0.0050877]\n",
      "[0.00488762]\n",
      "[0.00469686]\n",
      "[0.00451488]\n",
      "[0.0043412]\n",
      "[0.00417536]\n",
      "[0.00401693]\n",
      "[0.00386552]\n",
      "[0.00372076]\n",
      "[0.00358229]\n",
      "[0.00344979]\n",
      "[0.00332296]\n",
      "[0.0032015]\n",
      "[0.00308515]\n",
      "[0.00297365]\n",
      "[0.00286677]\n",
      "[0.00276427]\n",
      "[0.00266596]\n",
      "[0.00257162]\n",
      "[0.00248108]\n",
      "[0.00239415]\n",
      "[0.00231066]\n",
      "[0.00223046]\n",
      "[0.0021534]\n",
      "[0.00207934]\n",
      "[0.00200814]\n",
      "[0.00193967]\n",
      "[0.00187381]\n",
      "[0.00181045]\n",
      "[0.00174948]\n",
      "[0.0016908]\n",
      "[0.00163431]\n",
      "[0.00157992]\n",
      "[0.00152753]\n",
      "[0.00147707]\n",
      "[0.00142845]\n",
      "[0.00138159]\n",
      "[0.00133644]\n",
      "[0.0012929]\n",
      "[0.00125093]\n",
      "[0.00121045]\n",
      "[0.00117141]\n",
      "[0.00113375]\n",
      "[0.00109742]\n",
      "[0.98120457]\n",
      "[0.48253212]\n",
      "[0.31639647]\n",
      "[0.23339393]\n",
      "[0.18364377]\n",
      "[0.1505191]\n",
      "[0.12689412]\n",
      "[0.10920592]\n",
      "[0.09547515]\n",
      "[0.08451416]\n",
      "[0.07556723]\n",
      "[0.0681305]\n",
      "[0.06185519]\n",
      "[0.05649215]\n",
      "[0.05185869]\n",
      "[0.04781779]\n",
      "[0.04426467]\n",
      "[0.04111784]\n",
      "[0.03831297]\n",
      "[0.03579861]\n",
      "[0.03353308]\n",
      "[0.03148232]\n",
      "[0.02961817]\n",
      "[0.02791718]\n",
      "[0.02635963]\n",
      "[0.02492888]\n",
      "[0.02361071]\n",
      "[0.02239295]\n",
      "[0.02126513]\n",
      "[0.02021815]\n",
      "[0.0192441]\n",
      "[0.01833606]\n",
      "[0.01748794]\n",
      "[0.01669438]\n",
      "[0.01595063]\n",
      "[0.01525247]\n",
      "[0.01459613]\n",
      "[0.01397824]\n",
      "[0.01339578]\n",
      "[0.01284604]\n",
      "[0.01232657]\n",
      "[0.01183514]\n",
      "[0.01136975]\n",
      "[0.01092857]\n",
      "[0.01050994]\n",
      "[0.01011233]\n",
      "[0.00973437]\n",
      "[0.00937479]\n",
      "[0.0090324]\n",
      "[0.00870614]\n",
      "[0.00839503]\n",
      "[0.00809815]\n",
      "[0.00781466]\n",
      "[0.00754378]\n",
      "[0.00728478]\n",
      "[0.007037]\n",
      "[0.00679982]\n",
      "[0.00657266]\n",
      "[0.00635497]\n",
      "[0.00614626]\n",
      "[0.00594606]\n",
      "[0.00575392]\n",
      "[0.00556945]\n",
      "[0.00539224]\n",
      "[0.00522195]\n",
      "[0.00505823]\n",
      "[0.00490077]\n",
      "[0.00474927]\n",
      "[0.00460345]\n",
      "[0.00446304]\n",
      "[0.0043278]\n",
      "[0.00419749]\n",
      "[0.00407189]\n",
      "[0.00395079]\n",
      "[0.00383399]\n",
      "[0.00372131]\n",
      "[0.00361256]\n",
      "[0.00350758]\n",
      "[0.00340621]\n",
      "[0.00330831]\n",
      "[0.00321371]\n",
      "[0.0031223]\n",
      "[0.00303394]\n",
      "[0.00294851]\n",
      "[0.00286589]\n",
      "[0.00278597]\n",
      "[0.00270865]\n",
      "[0.00263382]\n",
      "[0.00256139]\n",
      "[0.00249126]\n",
      "[0.00242336]\n",
      "[0.00235759]\n",
      "[0.00229387]\n",
      "[0.00223214]\n",
      "[0.00217231]\n",
      "[0.00211432]\n",
      "[0.0020581]\n",
      "[0.00200359]\n",
      "[0.00195073]\n",
      "[0.97380278]\n",
      "[0.47542733]\n",
      "[0.30948244]\n",
      "[0.226642]\n",
      "[0.17704086]\n",
      "[0.14405734]\n",
      "[0.12056791]\n",
      "[0.10301084]\n",
      "[0.08940741]\n",
      "[0.07857044]\n",
      "[0.06974444]\n",
      "[0.06242581]\n",
      "[0.05626589]\n",
      "[0.05101567]\n",
      "[0.04649257]\n",
      "[0.04255964]\n",
      "[0.03911219]\n",
      "[0.0360688]\n",
      "[0.0333652]\n",
      "[0.03094998]\n",
      "[0.02878156]\n",
      "[0.02682588]\n",
      "[0.02505486]\n",
      "[0.02344508]\n",
      "[0.02197688]\n",
      "[0.02063364]\n",
      "[0.0194012]\n",
      "[0.01826743]\n",
      "[0.01722188]\n",
      "[0.0162555]\n",
      "[0.01536042]\n",
      "[0.01452974]\n",
      "[0.01375743]\n",
      "[0.01303813]\n",
      "[0.01236714]\n",
      "[0.01174027]\n",
      "[0.01115378]\n",
      "[0.01060433]\n",
      "[0.01008893]\n",
      "[0.0096049]\n",
      "[0.00914981]\n",
      "[0.00872148]\n",
      "[0.0083179]\n",
      "[0.0079373]\n",
      "[0.00757803]\n",
      "[0.00723859]\n",
      "[0.00691763]\n",
      "[0.00661389]\n",
      "[0.00632623]\n",
      "[0.00605361]\n",
      "[0.00579505]\n",
      "[0.00554967]\n",
      "[0.00531665]\n",
      "[0.00509522]\n",
      "[0.00488469]\n",
      "[0.00468441]\n",
      "[0.00449377]\n",
      "[0.00431222]\n",
      "[0.00413924]\n",
      "[0.00397433]\n",
      "[0.00381706]\n",
      "[0.00366699]\n",
      "[0.00352374]\n",
      "[0.00338694]\n",
      "[0.00325625]\n",
      "[0.00313134]\n",
      "[0.00301191]\n",
      "[0.00289769]\n",
      "[0.0027884]\n",
      "[0.00268379]\n",
      "[0.00258364]\n",
      "[0.00248771]\n",
      "[0.00239582]\n",
      "[0.00230774]\n",
      "[0.00222332]\n",
      "[0.00214236]\n",
      "[0.0020647]\n",
      "[0.0019902]\n",
      "[0.0019187]\n",
      "[0.00185007]\n",
      "[0.00178417]\n",
      "[0.00172088]\n",
      "[0.00166008]\n",
      "[0.00160166]\n",
      "[0.00154552]\n",
      "[0.00149155]\n",
      "[0.00143966]\n",
      "[0.00138976]\n",
      "[0.00134176]\n",
      "[0.00129559]\n",
      "[0.00125115]\n",
      "[0.00120839]\n",
      "[0.00116723]\n",
      "[0.0011276]\n",
      "[0.00108944]\n",
      "[0.00105268]\n",
      "[0.00101728]\n",
      "[0.00098317]\n",
      "[0.0009503]\n",
      "0.025264010023893384\n",
      "[('it didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt didnt', 'it didnt sound like a complaint exactly but just a way to let me know a kind of tender admission to remind me that he knew he was giving himself over to a voracious unfinished path that always required more .'), ('so its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its', 'so its different from the motor prosthetics where youre communicating from the brain to a device . here we have to communicate from the outside world into the brain and be understood and be understood by the brain .'), ('now let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let', 'now let me ask you this question this past week at ted how many of you when you saw vulnerability up here thought it was pure courage ?')]\n"
     ]
    }
   ],
   "source": [
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.beam_search(x), data['train'])\n",
    "print(bleu_eval(original, translations))\n",
    "print(list(zip(translations[0:3], original[0:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "input_seq = x['input']\n",
    "target_seq = x['target']\n",
    "input_length = x['input_length']\n",
    "target_length = x['target_length']\n",
    "\n",
    "encoded_input, hidden = translation_model.encoder.forward(input_seq, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(8.7238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.3296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.9268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.8758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.7349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.6699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.9962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.8494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.6858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.6537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.6393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.5373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.8509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-2)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(x)\n",
    "    loss = calc_loss(logits, x['target'], criterion)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-3)\n",
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34410a3c73c44d89f2e6cddd5d165c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1389), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.1450, device='cuda:0')\n",
      "0.0008142795832761162\n",
      "tensor(9.0464, device='cuda:0')\n",
      "0.0033784249080402885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bf6d2f87ea3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#evaluate(translation_model, data, data_loaders, dataset_type='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/PhD_Projects/nlpclass/nlpclass/models/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         decoder_outputs = Variable(torch.zeros(\n\u001b[0;32m--> 166\u001b[0;31m             model_config.max_length + 1, batch_size, self.decoder.output_size)).to(model_config.device)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm_notebook(data_loaders['train'])):\n",
    "    if i % 500 == 0:\n",
    "        evaluate(translation_model, data, data_loaders)\n",
    "        #evaluate(translation_model, data, data_loaders, dataset_type='train')\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(batch)\n",
    "    loss = calc_loss(logits, batch['target'], criterion)\n",
    "    loss.backward()\n",
    "    translation_model.encoder.embedding.weight.grad[data['train'].input_lang.pretrained_inds] = 0\n",
    "    translation_model.decoder.embedding.weight.grad[data['train'].output_lang.pretrained_inds] = 0\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_strings, translated_strings = evaluate(translation_model, data, data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball be later but of volumes all. hope. answer. only were',\n",
       " 'be this. 65. know what must truth. i most',\n",
       " 'games be practical everybody pins your needles all forward careful were',\n",
       " 'i moment. it attack. be sorrows. ten.',\n",
       " 'canadian be appointment careful laugh. no truth. something time were',\n",
       " 'beautiful cupboard. hot. wine these',\n",
       " 'much. but how bitter really dishonesty fed recently',\n",
       " 'applied all else fed to climate husbands. matter. key',\n",
       " 'but they answer. perfect. today. tape',\n",
       " 'i is invented i situation getting powers your few']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = translation_model.greedy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in predictions.cpu().numpy():\n",
    "    decoded_words = []\n",
    "    for elem in row[1:]:\n",
    "        decoded_words.append(data['train']['output_lang'].index2word[elem])\n",
    "        if elem == model_config.EOS_token:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = Variable(torch.LongTensor([model_config.SOS_token] * 8)).to(model_config.device)\n",
    "yo = torch.stack((yo, topi.squeeze(), topi.squeeze()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_range = torch.autograd.Variable(torch.LongTensor(np.repeat([2], len(x['input_length'])))).to(model_config.device)\n",
    "mask = seq_range < x['input_length']\n",
    "loss = -torch.gather(decoder_output, dim=1, index=input_var.unsqueeze(1)).squeeze() * mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4193, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.sum() / torch.sum(loss > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss > 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_output, encoder_hidden = encoder(x['input'], x['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = None\n",
    "if decoder.attention:\n",
    "    context = Variable(torch.zeros(encoder_output.size(0), encoder_output.size(2))).unsqueeze(1).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden, context, weights = decoder(input_var, encoder_hidden, encoder_output, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, criterion):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item() * \\\n",
    "            len(batch['label']) / len(train_loader.dataset)\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30a2084915f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-df8527ea006d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_loader, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(translation_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_model(translation_model, optimizer, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
