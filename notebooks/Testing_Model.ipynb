{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nlpclass.config import model_config\n",
    "from nlpclass.data.data_utils import TranslationDataset, text_collate_func\n",
    "from nlpclass.models.evaluation_utils import bleu_eval, output_to_translations\n",
    "from nlpclass.models.models import DecoderRNN, EncoderRNN, TranslationModel\n",
    "from nlpclass.models.training_utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_DIR = osp.join(CURRENT_PATH, '..', 'data')\n",
    "MODEL_DIR = osp.join(CURRENT_PATH, '..','models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "Counted words:\n",
      "eng 35150\n",
      "vi 43278\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3561\n",
      "vi 3677\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3360\n",
      "vi 3517\n"
     ]
    }
   ],
   "source": [
    "data, data_loaders, max_length = load_data('vi', batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(\n",
    "    data['train'].input_lang.n_words,\n",
    "    embedding_size=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True).to(model_config.device)\n",
    "if encoder.bidirectional:\n",
    "    multiplier = 2\n",
    "else:\n",
    "    multiplier = 1\n",
    "multiplier \n",
    "decoder = DecoderRNN(\n",
    "    embedding_size=128,\n",
    "    hidden_size=128 * multiplier,\n",
    "    output_size=data['train'].output_lang.n_words,\n",
    "    attention=True).to(model_config.device)\n",
    "translation_model = TranslationModel(encoder, decoder, teacher_forcing_ratio=0.5).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "input_seq = x['input']\n",
    "target_seq = x['target']\n",
    "input_length = x['input_length']\n",
    "target_length = x['target_length']\n",
    "\n",
    "encoded_input, hidden = translation_model.encoder.forward(input_seq, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, target, criterion):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_flat = target.view(-1, 1).squeeze()\n",
    "    return criterion(logits_flat, target_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(7.7296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.8561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.5984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.2933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.8101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.7192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.4480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-2)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(x)\n",
    "    loss = calc_loss(logits, x['target'], criterion)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00000000000004"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.greedy(x), data['train'])\n",
    "bleu_eval(original, translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, data_loaders, dataset_type='dev', max_batches=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        original_strings = []\n",
    "        translated_strings = []\n",
    "        for i, batch in enumerate(data_loaders[dataset_type]):\n",
    "            if i > max_batches:\n",
    "                break\n",
    "            logits = translation_model(batch)\n",
    "            epoch_loss = calc_loss(logits, batch['target'], criterion)\n",
    "            original = output_to_translations(batch['target'], data['train'])\n",
    "            translations = output_to_translations(model.greedy(batch), data['train'])\n",
    "            original_strings.extend(original)\n",
    "            translated_strings.extend(translations)\n",
    "        bleu = bleu_eval(original_strings, translated_strings)\n",
    "        model.train()\n",
    "        print(epoch_loss)\n",
    "        print(bleu)\n",
    "        \n",
    "        return original_strings, translated_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-4)\n",
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc5d399a86045a6ae3bebaec712ba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5555), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8823, device='cuda:0')\n",
      "6.206507731083603e-05\n",
      "tensor(9.5364, device='cuda:0')\n",
      "0.001002683570029698\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm_notebook(data_loaders['train'])):\n",
    "    if i % 250 == 0:\n",
    "        evaluate(translation_model, data, data_loaders)\n",
    "        #evaluate(translation_model, data, data_loaders, dataset_type='train')\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(batch)\n",
    "    loss = calc_loss(logits, batch['target'], criterion)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' quot gazelle i killed you for your skins exquisite touch for how easy it is to be nailed to a board weathered raw as white butcher paper .',\n",
       " 'heres why once they become ubiquitous each year these vehicles will save tens of thousands of lives in the united states alone and a million globally .',\n",
       " 'i was leaning against some sandbags one morning not much going on sort of spacing out and some sand was kicked into the side of sort of hit the side of my face .',\n",
       " 'hi . im going to ask you to raise your arms and wave back just the way i am kind of a royal wave .',\n",
       " 'and thus was born the project called remark which is a collaboration with zachary lieberman and the ars electronica futurelab .',\n",
       " 'more complex structures such as blood vessels urethras which i showed you theyre definitely more complex because youre introducing two different cell types .',\n",
       " 'theres an old african proverb that goes quot when spider webs unite they can halt even the lion . quot ',\n",
       " 'and indeed at the top youll see before the surgery the areas in blue are the areas that use less glucose than normal predominantly the parietal and temporal lobes .',\n",
       " 'with a thousand juli√°ns working together mexico would be a very different country .',\n",
       " ' quot terrace theater quot i actually put on the map because it wasnt on the map before ted last year .',\n",
       " 'here im taking a part of a document and putting over here a second part from a second place and im actually modifying the information that i have over there .',\n",
       " 'and were starting to move out of the just out of copyright into the out of print world .',\n",
       " 'you see thats the wonderful thing about the tidy up art idea its new . so there is no existing tradition in it .',\n",
       " 'yeah yo yo yo thank you . enjoy the rest . thank you .',\n",
       " 'and we watched them and im so thankful because shes just like quot wow ! this is so amazing . quot ',\n",
       " 'two different mythologies two different ways of looking at the world .',\n",
       " 'for africa to truly be sustainable we have to move beyond to other industries .',\n",
       " 'they can embed sensors and actuators right in the form itself .',\n",
       " 'its the entire history of my family in a single game .',\n",
       " 'this is expected to double triple or maybe quadruple year over year for the foreseeable future .',\n",
       " 'and tonight youll be able to see the device in the tent .',\n",
       " 'im going to try to push it inside your hand .',\n",
       " 'and this is when they think about beating the crap out of me after the talk .',\n",
       " 'it has just that sensor in it .',\n",
       " 'but you can see that no woman is standing on those .',\n",
       " 'we could produce better flavors and aromas .',\n",
       " 'i just want to know what the heck is it ?',\n",
       " 'its absolutely heartbreaking to see .',\n",
       " 'now ours is just one story .',\n",
       " 'i just tell the stories .',\n",
       " 'so its going to end .',\n",
       " 'it does not eat nectar .']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and you can see the first of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the united states and the next year .',\n",
       " 'and the other people have to be able to be able to make the same time and the world and the most of the world is the same of the world and the world .',\n",
       " 'and then i was a few years ago i was a lot of the first time i was a lot of the first time i was a lot of the time .',\n",
       " 'and so i want to show you a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit .',\n",
       " 'so the first thing is the first thing that is the first of the most of the most of the most of the most of the most of the most of the same time .',\n",
       " 'and you can see the other people who are the same time and you can see the other and the people who are the same time and you can see the other .',\n",
       " 'and the other thing that was the only one of the people who are the same time and they have to be a lot of the people who are the same time .',\n",
       " 'and the most important thing is that the same thing is that the same thing is the same thing that is the same thing that is the same thing .',\n",
       " 'if you have a lot of the brain and the most of the brain is the same thing that you can see the same .',\n",
       " 'i was a lot of the first time and i think that was a lot of people who are going to be a lot of people who are going to be a lot of them .',\n",
       " 'i was a lot of the first time that i was a lot of the things that i was in the last year .',\n",
       " 'we have a lot of the world and the world is that the same thing is the same thing .',\n",
       " 'you can see the world that you can see the world .',\n",
       " ' quot oh quot oh quot oh quot ',\n",
       " 'and we have a lot of her first time .',\n",
       " 'the world is the largest world in the world .',\n",
       " 'the other thing is that we can do it to make a lot of us .',\n",
       " 'you can see the same world in the united states and the world .',\n",
       " 'this is a very good thing i was a little bit of a little bit of a little bit .',\n",
       " 'and if you can see the world and the world is the same .',\n",
       " 'and you can see the other thing that you can see the other .',\n",
       " 'i want to do you to do you .',\n",
       " 'and this is a lot of my life .',\n",
       " 'its a lot of the world .',\n",
       " 'but you can see the other side of the other .',\n",
       " 'we can see the same thing that we can see the other .',\n",
       " 'i think that is what i think about it .',\n",
       " 'and it was a lot of things .',\n",
       " 'and we have a lot of this .',\n",
       " 'i think quot i was .',\n",
       " 'well its not .',\n",
       " 'and it was a lot of them .']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = translation_model.greedy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in predictions.cpu().numpy():\n",
    "    decoded_words = []\n",
    "    for elem in row[1:]:\n",
    "        decoded_words.append(data['train']['output_lang'].index2word[elem])\n",
    "        if elem == model_config.EOS_token:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = Variable(torch.LongTensor([model_config.SOS_token] * 8)).to(model_config.device)\n",
    "yo = torch.stack((yo, topi.squeeze(), topi.squeeze()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_range = torch.autograd.Variable(torch.LongTensor(np.repeat([2], len(x['input_length'])))).to(model_config.device)\n",
    "mask = seq_range < x['input_length']\n",
    "loss = -torch.gather(decoder_output, dim=1, index=input_var.unsqueeze(1)).squeeze() * mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4193, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.sum() / torch.sum(loss > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss > 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_output, encoder_hidden = encoder(x['input'], x['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = None\n",
    "if decoder.attention:\n",
    "    context = Variable(torch.zeros(encoder_output.size(0), encoder_output.size(2))).unsqueeze(1).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden, context, weights = decoder(input_var, encoder_hidden, encoder_output, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, criterion):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item() * \\\n",
    "            len(batch['label']) / len(train_loader.dataset)\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30a2084915f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-df8527ea006d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_loader, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(translation_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_model(translation_model, optimizer, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
