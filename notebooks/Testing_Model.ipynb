{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nlpclass.config import model_config\n",
    "from nlpclass.data.data_utils import TranslationDataset, text_collate_func\n",
    "from nlpclass.models.evaluation_utils import bleu_eval, output_to_translations\n",
    "from nlpclass.models.models import DecoderRNN, EncoderRNN, TranslationModel\n",
    "from nlpclass.models.training_utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_DIR = osp.join(CURRENT_PATH, '..', 'data')\n",
    "MODEL_DIR = osp.join(CURRENT_PATH, '..','models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "Counted words:\n",
      "eng 6872\n",
      "vi 7761\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3562\n",
      "vi 3678\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3361\n",
      "vi 3518\n"
     ]
    }
   ],
   "source": [
    "data, data_loaders, max_length = load_data('vi', batch_size=24, subsample=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(\n",
    "    data['train'].input_lang.n_words,\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True).to(model_config.device)\n",
    "if encoder.bidirectional:\n",
    "    multiplier = 2\n",
    "else:\n",
    "    multiplier = 1\n",
    "decoder = DecoderRNN(\n",
    "    embedding_size=100,\n",
    "    hidden_size=multiplier * 128,\n",
    "    output_size=data['train'].output_lang.n_words,\n",
    "    attention=True).to(model_config.device)\n",
    "translation_model = TranslationModel(encoder, decoder, teacher_forcing_ratio=0.5).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, target, criterion):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_flat = target.view(-1, 1).squeeze()\n",
    "    return criterion(logits_flat, target_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd79c1516e444bb7a0a7b3bf4454d6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12893779845325531 21.4522238945427 9.230515480041504\n",
      "0.0168476091610719 0.9833367456745542 8.915776252746582\n",
      "0.10603130909655946 4.09264806137575 8.88605785369873\n",
      "0.0220158280618112 0.9166286049260619 8.841546058654785\n",
      "0.03509080453019507 1.2450682199134009 8.71512508392334\n",
      "0.02542579718870091 1.2548735693619857 8.642480850219727\n",
      "0.05716306828953963 1.7156696152229607 8.567033767700195\n",
      "0.556621254967038 9.924774959720391 8.483844757080078\n",
      "0.21873765937856166 3.2314592226888035 8.031295776367188\n",
      "0.20114179194862383 3.4514577238566204 7.889610290527344\n",
      "0.03872926306872942 2.5025140574674163 7.734421730041504\n",
      "0.04148575138918659 2.1259934153251527 7.484665870666504\n",
      "0.04654673221451952 2.2293782398734825 7.456694602966309\n",
      "0.04377945915780031 1.964083057743635 7.196768760681152\n",
      "0.04190031092220785 2.0637321641482576 7.171753883361816\n",
      "0.042458468873247425 3.6984157287924786 7.170877933502197\n",
      "0.7219524070258759 20.528213052010955 7.433811187744141\n",
      "1.5675521020814844 390.84229090682953 7.231355667114258\n",
      "0.21272648426738883 60.81310514885183 7.055084705352783\n",
      "0.01296381653237379 1.424896238258377 7.263669013977051\n",
      "0.0324846525415179 2.1616581318770485 6.881598949432373\n",
      "0.006126930059751348 1.84868141584394 6.940064430236816\n",
      "0.038509461407334994 2.2933260074807653 6.738380432128906\n",
      "0.016919100019398954 2.128070735765195 6.974647521972656\n",
      "0.13440082908474388 2.647185190655612 6.965391159057617\n",
      "0.020520092561029303 1.4297408579779736 6.8789520263671875\n",
      "0.03936350672423502 1.563079410931729 6.824462413787842\n",
      "0.06321703783708892 2.3091177973635264 6.577685356140137\n",
      "0.008323676227237578 1.9176618886362762 6.782619953155518\n",
      "0.014440884419740136 1.4934506202283884 6.618374824523926\n",
      "0.00993819973539478 1.5482072115003747 6.769962787628174\n",
      "0.00809600478424577 1.783114147880977 6.479018688201904\n",
      "0.009496315424182687 1.9700243357296623 6.675850868225098\n",
      "0.01683876483738193 1.3376782113895036 6.580005645751953\n",
      "0.011345224419782345 2.008769669033955 6.725244522094727\n",
      "0.014337070123727013 1.672511707104374 6.801395893096924\n",
      "0.007734305473748445 1.2311262864112027 6.880416393280029\n",
      "0.011145759900158167 1.8769873343109338 6.697791576385498\n",
      "0.006722111944172603 2.0870208542981183 6.460004806518555\n",
      "0.009831547114449639 2.4529170512942726 6.58457088470459\n",
      "0.006516566827775813 1.4053669986988715 6.834421157836914\n",
      "0.009974341488624389 1.517386441568564 6.94827938079834\n",
      "0.012819662303861317 1.5669625243451515 6.650166988372803\n",
      "0.005664649072500561 1.9162768802846364 6.680549144744873\n",
      "0.005347886024978095 1.6145887466058648 6.6192193031311035\n",
      "0.01449529495459616 2.116985067158069 6.7593560218811035\n",
      "0.00852677765981762 1.9136255048204756 6.713033676147461\n",
      "0.008700761917245638 2.2999642469710064 6.680356502532959\n",
      "0.0075940817252669775 1.7179447154663576 6.429178237915039\n",
      "0.021606107001821 1.512825308186729 6.968575954437256\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(25)):\n",
    "    for batch in data_loaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        logits = translation_model(batch)\n",
    "        loss = calc_loss(logits, batch['target'], criterion)\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_norm = 0\n",
    "        for p in translation_model.encoder.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            encoder_norm += param_norm.item() ** 2\n",
    "        decoder_norm = 0\n",
    "        for p in translation_model.decoder.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            decoder_norm += param_norm.item() ** 2\n",
    "            \n",
    "        print(encoder_norm, decoder_norm, loss.item())\n",
    "            \n",
    "        clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                                   translation_model.parameters()), 5.0)\n",
    "        \n",
    "        optimizer.step() \n",
    "    original, translation = evaluate(translation_model, data, data_loaders, dataset_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.496940411158645\n",
      "[('and he was very well and and i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot i quot quot and i quot i quot and i quot i quot and i quot i quot and i quot i quot and i quot i quot quot', 'hes very very thin and he is indeed very sick with pneumonia and hes too sick to talk to me so i talk to his daughter kathleen and i say to her quot did you and jim ever talk about what you would want done if he ended up in this kind of situation ? quot '), ('and as a i i i like a a of of my my i could minutes i and and and', 'and what that is is i imagine explaining a work of art to my grandmother in five minutes and if i cant explain it in five minutes then its too obtuse or esoteric and it hasnt been refined enough yet .')]\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.greedy(x), data['train'])\n",
    "print(bleu_eval(original, translations))\n",
    "print(list(zip(translations[0:2], original[0:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002632711811722298\n",
      "['and its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its', 'and so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so', 'we just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just just', 'and was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was', 'so what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what', 'and they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they they', 'and he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he', 'so its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its its', 'so are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are', 'so was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was', 'and then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then then', 'why have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have have', 'and theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres theres', 'i girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl', 'so the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the', 'so to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to', 'and i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i', 'so it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it', 'they had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had had', 'and many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many many', 'so one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one', 'you if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if', 'so of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of', 'and that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that']\n"
     ]
    }
   ],
   "source": [
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.beam_search(x), data['train'])\n",
    "print(bleu_eval(original, translations))\n",
    "print(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "input_seq = x['input']\n",
    "target_seq = x['target']\n",
    "input_length = x['input_length']\n",
    "target_length = x['target_length']\n",
    "\n",
    "encoded_input, hidden = translation_model.encoder.forward(input_seq, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6125, -0.5094,  0.1136,  ...,  0.2936, -0.0332, -0.5566],\n",
       "        [-0.2966, -0.5157,  0.0630,  ...,  0.6734,  0.0263, -0.9648],\n",
       "        [ 0.3257, -1.1140,  0.3586,  ...,  0.7310, -0.7097, -0.5748],\n",
       "        ...,\n",
       "        [-0.3053, -0.1229,  0.2143,  ..., -0.5936, -0.5418, -0.0655],\n",
       "        [-0.2868, -0.2641,  0.6324,  ..., -0.5235, -0.4746, -0.1487],\n",
       "        [-0.0230, -0.0655,  0.1169,  ..., -0.4210,  0.3063,  0.4462]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(9.9659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(8.5004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(7.8864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.6769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.8524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.8219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.5111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.7274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.8735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.7627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.8793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-2)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(x)\n",
    "    loss = calc_loss(logits, x['target'], criterion)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    translation_model.encoder.embedding.weight.grad[data['train'].input_lang.pretrained_inds] = 0\n",
    "    translation_model.decoder.embedding.weight.grad[data['train'].output_lang.pretrained_inds] = 0\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, data_loaders, dataset_type='dev', max_batches=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        original_strings = []\n",
    "        translated_strings = []\n",
    "        for i, batch in enumerate(data_loaders[dataset_type]):\n",
    "            if i > max_batches:\n",
    "                break\n",
    "            logits = translation_model(batch)\n",
    "            epoch_loss = calc_loss(logits, batch['target'], criterion)\n",
    "            original = output_to_translations(batch['target'], data['train'])\n",
    "            translations = output_to_translations(model.greedy(batch), data['train'])\n",
    "            original_strings.extend(original)\n",
    "            translated_strings.extend(translations)\n",
    "        bleu = bleu_eval(original_strings, translated_strings)\n",
    "        model.train()\n",
    "        print(epoch_loss)\n",
    "        print(bleu)\n",
    "        \n",
    "        return original_strings, translated_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-3)\n",
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34410a3c73c44d89f2e6cddd5d165c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1389), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.1450, device='cuda:0')\n",
      "0.0008142795832761162\n",
      "tensor(9.0464, device='cuda:0')\n",
      "0.0033784249080402885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bf6d2f87ea3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#evaluate(translation_model, data, data_loaders, dataset_type='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/PhD_Projects/nlpclass/nlpclass/models/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         decoder_outputs = Variable(torch.zeros(\n\u001b[0;32m--> 166\u001b[0;31m             model_config.max_length + 1, batch_size, self.decoder.output_size)).to(model_config.device)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm_notebook(data_loaders['train'])):\n",
    "    if i % 500 == 0:\n",
    "        evaluate(translation_model, data, data_loaders)\n",
    "        #evaluate(translation_model, data, data_loaders, dataset_type='train')\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(batch)\n",
    "    loss = calc_loss(logits, batch['target'], criterion)\n",
    "    loss.backward()\n",
    "    translation_model.encoder.embedding.weight.grad[data['train'].input_lang.pretrained_inds] = 0\n",
    "    translation_model.decoder.embedding.weight.grad[data['train'].output_lang.pretrained_inds] = 0\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_strings, translated_strings = evaluate(translation_model, data, data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball be later but of volumes all. hope. answer. only were',\n",
       " 'be this. 65. know what must truth. i most',\n",
       " 'games be practical everybody pins your needles all forward careful were',\n",
       " 'i moment. it attack. be sorrows. ten.',\n",
       " 'canadian be appointment careful laugh. no truth. something time were',\n",
       " 'beautiful cupboard. hot. wine these',\n",
       " 'much. but how bitter really dishonesty fed recently',\n",
       " 'applied all else fed to climate husbands. matter. key',\n",
       " 'but they answer. perfect. today. tape',\n",
       " 'i is invented i situation getting powers your few']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = translation_model.greedy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in predictions.cpu().numpy():\n",
    "    decoded_words = []\n",
    "    for elem in row[1:]:\n",
    "        decoded_words.append(data['train']['output_lang'].index2word[elem])\n",
    "        if elem == model_config.EOS_token:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = Variable(torch.LongTensor([model_config.SOS_token] * 8)).to(model_config.device)\n",
    "yo = torch.stack((yo, topi.squeeze(), topi.squeeze()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_range = torch.autograd.Variable(torch.LongTensor(np.repeat([2], len(x['input_length'])))).to(model_config.device)\n",
    "mask = seq_range < x['input_length']\n",
    "loss = -torch.gather(decoder_output, dim=1, index=input_var.unsqueeze(1)).squeeze() * mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4193, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.sum() / torch.sum(loss > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss > 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_output, encoder_hidden = encoder(x['input'], x['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = None\n",
    "if decoder.attention:\n",
    "    context = Variable(torch.zeros(encoder_output.size(0), encoder_output.size(2))).unsqueeze(1).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden, context, weights = decoder(input_var, encoder_hidden, encoder_output, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, criterion):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item() * \\\n",
    "            len(batch['label']) / len(train_loader.dataset)\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30a2084915f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-df8527ea006d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_loader, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(translation_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_model(translation_model, optimizer, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
