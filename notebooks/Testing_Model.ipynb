{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nlpclass.config import model_config\n",
    "from nlpclass.data.data_utils import TranslationDataset, text_collate_func\n",
    "from nlpclass.models.evaluation_utils import bleu_eval, output_to_translations\n",
    "from nlpclass.models.models import DecoderRNN, EncoderCNN, EncoderRNN, TranslationModel\n",
    "from nlpclass.models.training_utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_DIR = osp.join(CURRENT_PATH, '..', 'data')\n",
    "MODEL_DIR = osp.join(CURRENT_PATH, '..','models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "Counted words:\n",
      "eng 6948\n",
      "vi 7864\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3562\n",
      "vi 3678\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3361\n",
      "vi 3518\n"
     ]
    }
   ],
   "source": [
    "data, data_loaders, max_length = load_data('vi', batch_size=24, subsample=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(\n",
    "    data['train'].input_lang.n_words,\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2).to(model_config.device)\n",
    "decoder = DecoderRNN(\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    output_size=data['train'].output_lang.n_words,\n",
    "    attention=False).to(model_config.device)\n",
    "translation_model = TranslationModel(encoder, decoder, teacher_forcing_ratio=0.5).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(\n",
    "    data['train'].input_lang.n_words,\n",
    "    embedding_size=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True).to(model_config.device)\n",
    "if encoder.bidirectional:\n",
    "    multiplier = 2\n",
    "else:\n",
    "    multiplier = 1\n",
    "decoder = DecoderRNN(\n",
    "    embedding_size=100,\n",
    "    hidden_size=multiplier * 128,\n",
    "    output_size=data['train'].output_lang.n_words,\n",
    "    attention=True).to(model_config.device)\n",
    "translation_model = TranslationModel(encoder, decoder, teacher_forcing_ratio=0.5).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, target, criterion):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_flat = target.view(-1, 1).squeeze()\n",
    "    return criterion(logits_flat, target_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d478dee26954f86aad5c9d1d3281627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003903319319946857 0.3320062164179496 6.233051776885986\n",
      "0.0029954760657136576 0.3045315754294484 6.101306438446045\n",
      "0.006289633391112626 0.3848233708385577 6.138204097747803\n",
      "0.0030441041977380024 0.23898369123124008 6.211655139923096\n",
      "0.005517929261783358 0.39761350173848364 6.114835262298584\n",
      "0.006022005998200187 0.4207513659216756 6.155610084533691\n",
      "0.004927983310479664 0.436079190623927 6.072620391845703\n",
      "0.004511587985125415 0.3095739289143282 6.404386520385742\n",
      "0.007172139693399509 0.33974534864295053 5.984451770782471\n",
      "0.005394037148850503 0.362960477815355 6.068565368652344\n",
      "0.004844854610884176 0.3816862397954417 6.51413106918335\n",
      "0.0038207866004794635 0.4991620759250848 6.330790042877197\n",
      "0.003370597843022552 0.26525671842023263 6.196446418762207\n",
      "0.002678056877915128 0.24418903662384453 6.1278533935546875\n",
      "0.004902798979096537 0.3594419084457389 6.119089603424072\n",
      "0.006206152903313689 0.7092265024079555 6.329479694366455\n",
      "0.007773167685064254 0.4654935835589584 5.968400001525879\n",
      "0.005164353204756673 0.34374470198399215 6.0751872062683105\n",
      "0.0033468733902686874 0.49137852433895085 6.264949798583984\n",
      "0.005470200589560043 0.32867993250890626 6.26352071762085\n",
      "0.00604504062698865 0.37650178650628247 6.598834037780762\n",
      "0.0065844594389578015 0.4097668081929181 6.461611747741699\n",
      "0.008132372865928075 0.4639595194610576 5.910901069641113\n",
      "0.0038944284062353007 0.32724882281560874 6.1281538009643555\n",
      "0.00515828304236061 0.3204953062736621 6.062570095062256\n",
      "0.007207689466551637 0.3403689500065537 6.425436973571777\n",
      "0.004520064911284166 0.3669257117039066 6.2109880447387695\n",
      "0.005115241298709653 0.3288501375438742 6.367339134216309\n",
      "0.006869002165279469 0.35080484195133993 6.268728733062744\n",
      "0.005712943344388036 0.386318742407375 6.011596202850342\n",
      "0.004271522498021464 0.2891697993108631 6.378009796142578\n",
      "0.007136424968738895 0.37794516224290875 6.286101341247559\n",
      "0.004422834016173146 0.3871742590188189 5.941765785217285\n",
      "0.005991597974453085 0.2726946704953094 6.378477096557617\n",
      "0.006133247689535726 0.4273100961027895 5.848509788513184\n",
      "0.007897993568250005 0.3125994615879616 6.2397966384887695\n",
      "0.008059729009157371 0.3810555273701415 5.926957607269287\n",
      "0.003721003250378008 0.30259327126603164 6.414824962615967\n",
      "0.007056471978036377 0.311181200310113 6.11638069152832\n",
      "0.004637721224142936 0.281993983631677 6.347616195678711\n",
      "0.00643376483119329 0.39451193143047286 6.002289295196533\n",
      "0.004697294952495538 0.37465793389530333 6.483653545379639\n",
      "0.005679836205102478 0.41385931987721053 6.514498710632324\n",
      "0.011422477677088106 0.4176108750816386 6.32985258102417\n",
      "0.01075903195463487 0.5920167639996637 5.761227607727051\n",
      "0.007878056592321265 0.37339395368940437 6.184809684753418\n",
      "0.005890819219223899 0.38944669440296736 6.338047504425049\n",
      "0.004707631417007113 0.30835412512751603 6.069521903991699\n",
      "0.008091853567962926 0.4010354276092771 6.100844383239746\n",
      "0.008896293690443409 0.394545277687065 6.375974178314209\n",
      "0.004941949307460753 0.2886069522139908 6.474092483520508\n",
      "0.005694935347797615 0.29413843024317193 6.157577991485596\n",
      "0.00782368054869369 0.33497831792795874 6.448349475860596\n",
      "0.01154738292630203 0.4167738349460489 6.101107597351074\n",
      "0.008558038103282047 0.39958908973987367 6.1092848777771\n",
      "0.005769893598475757 0.448576363721736 6.32198429107666\n",
      "0.005478346011561521 0.28718080078464053 6.0357346534729\n",
      "0.004998334964543887 0.27200669440741465 6.263291835784912\n",
      "0.009005519939976867 0.4118968101197404 5.787919998168945\n",
      "0.008491530522213237 0.32558962679887143 6.073693752288818\n",
      "0.010688590655945391 0.4825287914476879 6.200258731842041\n",
      "0.004760878573450548 0.2897669349169195 5.8542304039001465\n",
      "0.004733453250496492 0.2507376643932854 6.14005184173584\n",
      "0.005328717326003254 0.6086344409007708 6.464848041534424\n",
      "0.007529787994160302 0.42920935815591366 5.979588031768799\n",
      "0.007304931281289578 0.6528721518361854 6.365600109100342\n",
      "0.004768824376753415 0.9595521957092787 6.669226169586182\n",
      "0.0075948257800804945 0.33035174355672176 5.956376552581787\n",
      "0.006578828611107047 0.47340978047139987 6.26262092590332\n",
      "0.004955008268348376 0.2946660887056016 6.09210205078125\n",
      "0.007652066999173952 0.32726602441737307 6.241386413574219\n",
      "0.0038572578320022386 0.27567179944878284 6.227734088897705\n",
      "0.008222704781951605 0.46762038212707624 6.020767688751221\n",
      "0.009319449791085566 0.3554473945255025 6.291727066040039\n",
      "0.005892229196314424 0.23924726416196723 6.3101701736450195\n",
      "0.006552399717765632 0.2509959753951885 6.386040210723877\n",
      "0.007269466631723901 0.39902769410618855 6.071024417877197\n",
      "0.017637376113198704 0.471073389064269 6.252848148345947\n",
      "0.006523552113710113 0.34798129786677556 6.518226146697998\n",
      "0.005893390584549038 0.25852921707921095 6.460390567779541\n",
      "0.007391331121076694 0.3695634535194644 6.240475654602051\n",
      "0.006842303684952684 0.2602214349061566 6.531630992889404\n",
      "0.012986760081101077 0.4750004049746513 6.095747470855713\n",
      "0.008897539616724242 0.38600925765976873 6.153797149658203\n",
      "0.006389407253324315 0.30282170476833387 6.1881794929504395\n",
      "0.009915837720956833 0.37392734699166 6.09996223449707\n",
      "0.007315075464065129 0.25502042792209584 6.171780586242676\n",
      "0.008239142508256586 0.46277576287379907 6.1543707847595215\n",
      "0.008077883541841522 0.36349018617396867 5.964039325714111\n",
      "0.01144007674027239 0.37413884332048075 6.032814025878906\n",
      "0.010453567349622543 0.4355836939057506 6.330183029174805\n",
      "0.0069121775768291125 0.4149708563114326 6.37047004699707\n",
      "0.008836318863391633 0.3599616260373966 5.904957294464111\n",
      "0.01191561282673103 0.380695863284938 6.184137344360352\n",
      "0.007284952125199126 0.4625558135524929 6.596401691436768\n",
      "0.004695719299214356 0.5511324218621717 6.395468235015869\n",
      "0.007932387222259333 0.35670403318108 6.192157745361328\n",
      "0.006677894305985799 0.3389537891579628 6.058900356292725\n",
      "0.006656067774937488 0.3506869577929263 6.633960723876953\n",
      "0.010185744029554394 0.35269872521524537 6.289316177368164\n",
      "0.010891351140649194 0.5136262733670298 6.125096321105957\n",
      "0.009327747201937537 0.3447089069690073 6.417238712310791\n",
      "0.005541289922937633 0.2369162404622045 6.255131721496582\n",
      "0.012405185142687286 0.3500531808180317 6.136301517486572\n",
      "0.005888836306342422 0.41982436153782854 6.444929599761963\n",
      "0.009609185361767787 0.3768685513052196 5.850405216217041\n",
      "0.006276007363793899 0.3024022260944413 6.353789329528809\n",
      "0.011068715896017274 0.356530583197139 6.295229911804199\n",
      "0.011087449023202117 0.3670523822883306 6.275043487548828\n",
      "0.01118146808217836 0.4842057805017436 6.136042594909668\n",
      "0.00841219037738154 0.3007577382982581 6.573532581329346\n",
      "0.007699364590929632 0.3496321332120994 5.805920124053955\n",
      "0.009035464947912346 0.3714879113062628 5.916396141052246\n",
      "0.009124336428587895 0.4335793346046623 5.911333084106445\n",
      "0.009851212072607116 0.4048719433505394 6.010257720947266\n",
      "0.014403272745912637 0.45077309066701915 5.909640312194824\n",
      "0.011889805039458702 0.39196133711019776 5.758113384246826\n",
      "0.005080996793139045 0.388148869958039 6.2624945640563965\n",
      "0.005634688716548926 0.5164442043580062 6.288781642913818\n",
      "0.008558396299220332 0.45891408553131596 6.401782512664795\n",
      "0.015587225352919612 0.4585249802778482 5.772140979766846\n",
      "0.01317046302792764 0.3696825317326497 6.289907932281494\n",
      "0.007394420357785062 0.4051575490303808 6.515387058258057\n",
      "0.005648322840376946 0.36075440477087667 6.42821741104126\n",
      "0.005012674891922577 0.28492143306407464 6.21985387802124\n",
      "0.006405946620911262 0.24961071982366662 6.286136150360107\n",
      "0.0036092750709050417 0.2822904683788169 6.606803894042969\n",
      "0.006232599588374692 0.2550218607306278 6.289486408233643\n",
      "0.011356381959613998 0.37550941734521887 5.998922348022461\n",
      "0.013482354677193081 0.5161133320587752 5.79178524017334\n",
      "0.00653868330584537 0.2551952678775292 6.217371463775635\n",
      "0.006141231490662974 0.25115522204920454 6.379676818847656\n",
      "0.00790865889846001 0.2569377840997878 6.624264240264893\n",
      "0.008017723182213447 0.2757212651484521 6.162881374359131\n",
      "0.007947922368185185 0.2945737007936447 6.283438682556152\n",
      "0.005759775725249179 0.20822161884060722 6.317991733551025\n",
      "0.007452301999921513 0.3472815824021084 5.807028293609619\n",
      "0.004980009706550991 0.27635468189824974 6.135039329528809\n",
      "0.006726105901259335 0.33100408876291865 6.178455829620361\n",
      "tensor(6.2826, device='cuda:0')\n",
      "0.18986009667146003\n",
      "0.008958575135530616 0.33549018034566674 6.063475131988525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007119436857271959 0.36043198366327034 5.803203582763672\n",
      "0.008171087953914778 0.2905502916432191 6.059947490692139\n",
      "0.003908692731832899 0.2554137310300834 6.019806861877441\n",
      "0.007480740298017251 0.29647591286006353 6.333374500274658\n",
      "0.006088185059202812 0.27583103276838394 6.132862091064453\n",
      "0.008461604704472252 0.34969169383149257 6.1865234375\n",
      "0.009890092446665668 0.380367035530405 5.5932416915893555\n",
      "0.004475731216877255 0.3286145131391133 6.424455165863037\n",
      "0.006243362032695032 0.2744235360601953 6.02078914642334\n",
      "0.00709809079760736 0.45089838750539585 5.807844638824463\n",
      "0.0059387513988864 0.29330113205828057 6.350605010986328\n",
      "0.006881535091226259 0.3353357975040091 5.947839736938477\n",
      "0.007084215597305712 0.2751289657568735 5.872499942779541\n",
      "0.011322695846346296 0.34426936008430686 5.807190418243408\n",
      "0.008525638525082676 0.4698375446419751 6.548463821411133\n",
      "0.011202150100106242 0.40011719547668306 5.604949951171875\n",
      "0.007305019934827224 0.28194256981911414 6.247852325439453\n",
      "0.010443616746953734 0.28185128228000716 6.3224334716796875\n",
      "0.01195838651204651 0.44163752956510477 5.767967700958252\n",
      "0.014839466089688686 0.34631303833995175 6.0948100090026855\n",
      "0.009647253128865541 0.31348740278856585 5.626239776611328\n",
      "0.008320918174854034 0.2802885393811547 5.720312118530273\n",
      "0.011057709037359968 0.3113476284073105 6.389060974121094\n",
      "0.0065951243590924 0.28767450866899774 6.244976997375488\n",
      "0.007915020600472607 0.3051594086215006 6.173730373382568\n",
      "0.008114266878894938 0.2871452519255894 5.818732261657715\n",
      "0.01083726720012291 0.4739791058946313 5.732669353485107\n",
      "0.007953177843014745 0.267391220463986 6.337075233459473\n",
      "0.015785655537938946 0.5074100005949446 6.214050769805908\n",
      "0.013097762342960879 0.2982803214408646 6.254965782165527\n",
      "0.009466328779914704 0.35646895352857133 5.96209716796875\n",
      "0.01443748924915591 0.37086551082820696 5.697871208190918\n",
      "0.006444834025630074 0.2862818289166039 6.396867275238037\n",
      "0.014270606882547663 0.35062889415592224 5.742604732513428\n",
      "0.013185881820374867 0.36024097163354574 5.6997857093811035\n",
      "0.010492276631041877 0.351662306964254 6.447885036468506\n",
      "0.012797186010330405 0.3323647824732738 6.190519332885742\n",
      "0.013197416922116229 0.3735249293226503 6.314482688903809\n",
      "0.00826311531653448 0.3317967272913373 6.323742389678955\n",
      "0.006961970683883838 0.34164200213807344 6.242188453674316\n",
      "0.00927058564717904 0.361823842379981 5.785196781158447\n",
      "0.007400110435416463 0.27034833789078266 6.426319599151611\n",
      "0.004720057612244581 0.29006950335590725 6.062857151031494\n",
      "0.00846690501649034 0.31249689995368557 6.427820205688477\n",
      "0.014445156137203588 0.3951450915952176 5.758059024810791\n",
      "0.010102844727678936 0.30498221919047935 5.781245231628418\n",
      "0.01005907397105113 0.3665037983672358 5.831788063049316\n",
      "0.009705476063966666 0.29690677991136566 6.273730278015137\n",
      "0.014526538212400342 0.35598431345991216 6.148992538452148\n",
      "0.011081163867950102 0.34203014026497963 6.1279683113098145\n",
      "0.017970228223067194 0.4946727520034526 6.18277645111084\n",
      "0.013440881987378814 0.3498420626011665 5.922097682952881\n",
      "0.017114749270730957 0.46074398205232203 6.305150508880615\n",
      "0.018956983388868347 0.3165728097148137 5.907136917114258\n",
      "0.010451838796555339 0.3816808087610292 6.33959436416626\n",
      "0.006741510515524499 0.3065623683869359 6.241547107696533\n",
      "0.0198750888491017 0.443401740883778 6.117748737335205\n",
      "0.009893282450985703 0.3316987583886452 6.255133152008057\n",
      "0.01571977275953506 0.3400080676593699 6.343877792358398\n",
      "0.00831187142006745 0.28188617702404734 6.238253593444824\n",
      "0.00910746821172979 0.32388530107641617 6.433444023132324\n",
      "0.009000847198479467 0.2940435494037647 5.829497337341309\n",
      "0.00893052604931004 0.3203865887817938 6.232264518737793\n",
      "0.010524272628341417 0.25861846627386753 5.889214038848877\n",
      "0.00961846899559479 0.23035632539723147 6.066636085510254\n",
      "0.0075065936895897375 0.2793726267522278 5.96525239944458\n",
      "0.014899654790097045 0.3144984758214991 6.3663153648376465\n",
      "0.016786479801239575 0.4042651102361232 5.790221214294434\n",
      "0.0193256346569503 0.49155929293466954 5.699143409729004\n",
      "0.012114881530851794 0.3923773903189978 6.148510456085205\n",
      "0.021061220907643945 0.5539927475208356 6.13068151473999\n",
      "0.01430414582127075 0.35821610467406917 6.1521897315979\n",
      "0.014879222634636417 0.3507803339983411 5.824734210968018\n",
      "0.012021044462660087 0.35623692872914264 5.961086273193359\n",
      "0.01207129937324517 0.3184352011840792 6.103438854217529\n",
      "0.016321983496990156 0.3764895852202932 6.196660041809082\n",
      "0.009336609493014528 0.29023287080676646 5.7965006828308105\n",
      "0.015919878561188785 0.392633480966027 6.3646345138549805\n",
      "0.01407065020721648 0.32238421172275056 5.980144023895264\n",
      "0.013641706279211064 0.36440815407619814 5.882532119750977\n",
      "0.011470274719370344 0.36965840124980726 6.322892665863037\n",
      "0.008945122304813112 0.3015940464279814 5.567937850952148\n",
      "0.017406793580434843 0.41738955457800875 5.574697971343994\n",
      "0.016006766962270543 0.3875272250160039 6.205780982971191\n",
      "0.005963828389425402 0.32184738731173407 6.4473161697387695\n",
      "0.010681443220469777 0.3142996563026219 5.7411723136901855\n",
      "0.010566848900676983 0.3455848061813897 5.689084053039551\n",
      "0.008179013131172237 0.3451089454113828 6.331557750701904\n",
      "0.02085995480585243 0.5208483630025624 5.455727577209473\n",
      "0.012961510956400125 0.29761181815704635 5.660314559936523\n",
      "0.014372413956406462 0.38061291392209623 6.612232208251953\n",
      "0.011848082621910649 0.3197736093913489 6.298322677612305\n",
      "0.017301062446435665 0.4267684804293998 5.732916831970215\n",
      "0.0154195626875591 0.3458317376203855 6.389995574951172\n",
      "0.011011043842438742 0.32064366761850105 6.235968589782715\n",
      "0.014479956027175401 0.40892453494465797 5.936785697937012\n",
      "0.01213695867989082 0.2924708099216686 5.741312026977539\n",
      "0.016863756472043936 0.3808206542544725 6.261542797088623\n",
      "0.009928185775300599 0.26349326741901485 5.590195178985596\n",
      "0.011905086239495787 0.38733190278849217 5.933387756347656\n",
      "0.008957779849146351 0.2984168681419554 6.147327899932861\n",
      "0.007754763969884263 0.26545246650082616 6.645322322845459\n",
      "0.015308723149450839 0.3806694163560733 5.884828090667725\n",
      "0.014231788046959938 0.41660102105766184 6.1657843589782715\n",
      "0.015105243162652899 0.33042105655502285 6.300447463989258\n",
      "0.012139249070227894 0.28621249616460925 6.278590202331543\n",
      "0.013213036889239187 0.30148873784946845 5.793309688568115\n",
      "0.010894258926246216 0.3028602334431778 5.7833051681518555\n",
      "0.01647544861097952 0.353178715712278 6.147356986999512\n",
      "0.009492386468981987 0.3102247085158269 6.318373203277588\n",
      "0.011849048950629281 0.3049034621825694 5.611772537231445\n",
      "0.01042882785157748 0.2581039497928262 6.276552200317383\n",
      "0.013146027440423206 0.4162751812211515 6.471765041351318\n",
      "0.008098979313784588 0.4865306350794465 5.721336841583252\n",
      "0.010314355868440322 0.2867406030702603 6.048243045806885\n",
      "0.013939851352361927 0.33279336889545796 5.783207416534424\n",
      "0.00969892114458993 0.2899305487557812 6.098340034484863\n",
      "0.014689219229459278 0.31987379771575875 5.66648530960083\n",
      "0.013372842162860386 0.30843615049601075 5.73842716217041\n",
      "0.01195127494360855 0.25336660500273644 5.905160427093506\n",
      "0.019280144628789447 0.47794172732860557 6.349470138549805\n",
      "0.012782037501658112 0.2840723129075289 6.092629909515381\n",
      "0.01163409385425651 0.2779197778860095 5.58611536026001\n",
      "0.012733905048799739 0.33441782623879845 5.698766231536865\n",
      "0.009954248793808379 0.3071473121127408 5.702389717102051\n",
      "0.009101807287960683 0.31568750819408975 6.095938205718994\n",
      "0.010675278115841852 0.29106289052380896 6.483423709869385\n",
      "0.008361475739157812 0.2570486450651155 5.745460033416748\n",
      "0.017364462265243918 0.39165725557453 5.9880852699279785\n",
      "0.014851974540215243 0.298891596898941 5.695068359375\n",
      "0.01780013478709387 0.29763600158006087 6.173855304718018\n",
      "0.011855073916256054 0.30208861207510207 6.197234153747559\n",
      "0.011559863951394914 0.310540985911219 5.555905818939209\n",
      "0.010871274113661008 0.30363444664330236 5.658147811889648\n",
      "0.009210150688395174 0.2697999359434121 5.844188213348389\n",
      "0.0073734736421785345 0.300226475129291 6.493691444396973\n",
      "0.014987528430310478 0.3066981177932438 5.587707996368408\n",
      "0.027862557091729814 0.5626726284597728 5.968784809112549\n",
      "tensor(6.2646, device='cuda:0')\n",
      "0.047545627120879216\n",
      "0.011154761454096501 0.3715372237120496 5.526025772094727\n",
      "0.007279407021391616 0.2999580800203296 5.527298450469971\n",
      "0.008434924414911109 0.26800330595822847 5.730958461761475\n",
      "0.008165174782928102 0.3226593881341654 6.137477397918701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01064453190095992 0.3079182065862358 6.225142478942871\n",
      "0.0072761963912239625 0.2685474636559837 5.630947113037109\n",
      "0.011357519725861194 0.31298296245150603 6.3709716796875\n",
      "0.011580004278974126 0.32726245626604455 6.04130220413208\n",
      "0.011225051103405157 0.3643128681333989 6.220548629760742\n",
      "0.01192180499956758 0.36268286967930774 6.197338581085205\n",
      "0.009372664841075101 0.44496156567288375 6.371781826019287\n",
      "0.009585115060581389 0.2670841092819331 5.821117877960205\n",
      "0.016129001678563817 0.4229237053351461 6.375631332397461\n",
      "0.010801401424838692 0.33623278728396616 6.550741672515869\n",
      "0.010147835143328227 0.27560503325884456 5.709831714630127\n",
      "0.007454408912574523 0.27204669969015133 5.793054580688477\n",
      "0.01566021969297946 0.41682858247821214 5.897654056549072\n",
      "0.009218386520753548 0.31660827512706796 6.344803333282471\n",
      "0.011845894193414327 0.3871606728825352 5.854465961456299\n",
      "0.00724105562505658 0.2611200526905314 6.148344039916992\n",
      "0.014482901594270976 0.3591424285775552 5.542802333831787\n",
      "0.009049414815318861 0.384399675105901 5.832316875457764\n",
      "0.00979837098050906 0.3750017792500591 5.4661545753479\n",
      "0.013636662662736235 0.3847368435403405 5.704859733581543\n",
      "0.02011301290184177 0.6351446510552465 6.127893447875977\n",
      "0.00691728799578114 0.33515534163596294 5.943752288818359\n",
      "0.009486992988160031 0.27887424718050136 6.306532382965088\n",
      "0.01358969233369046 0.345941111823723 6.150126934051514\n",
      "0.01720819948190668 0.47819981810117185 6.172581195831299\n",
      "0.009329426352734406 0.26694255097685093 5.875011444091797\n",
      "0.010422937306077774 0.33841861475972773 6.205294609069824\n",
      "0.01683545543477579 0.3753760423905071 6.052069664001465\n",
      "0.016640629963110307 0.3189300198569661 5.760446071624756\n",
      "0.012240080993791754 0.36688931939385006 5.456973552703857\n",
      "0.014896294441457243 0.27794833969302846 5.6355791091918945\n",
      "0.023227643349643973 0.4642553307785629 6.072753429412842\n",
      "0.020622634026832485 0.46023286045415196 5.986989974975586\n",
      "0.0095665517826829 0.2762070041456217 5.826968193054199\n",
      "0.016287752696553047 0.4932635931696585 5.555943489074707\n",
      "0.01779421815595179 0.39958406881247677 6.033397674560547\n",
      "0.017501660250353393 0.4085374607050151 5.909401893615723\n",
      "0.01022337593057336 0.2975426644258761 5.86260461807251\n",
      "0.025593988658768316 0.5845002615993133 5.247668266296387\n",
      "0.01797596234706016 0.3939160150194615 5.6616387367248535\n",
      "0.017151997773643915 0.38698749034319563 5.761486053466797\n",
      "0.01998050523582194 0.555776745663116 6.058704376220703\n",
      "0.018238377587820778 0.42283837596111856 6.0988922119140625\n",
      "0.008629516524357181 0.29697493365170974 5.672961711883545\n",
      "0.01236579068487043 0.3608696291493152 6.2867350578308105\n",
      "0.010271826175476003 0.42953675769759614 6.264007568359375\n",
      "0.01852531820856922 0.4060029669671353 6.032486915588379\n",
      "0.013787501048004172 0.4277077059421731 5.896743297576904\n",
      "0.010787992189559883 0.34296928905330537 6.067294597625732\n",
      "0.020409305805043385 0.40179407987506516 6.073215484619141\n",
      "0.021259315619901395 0.4864957894738541 5.73089075088501\n",
      "0.018501523548300303 0.510080281350354 5.848306179046631\n",
      "0.0154135328553886 0.3544915694745806 5.872152805328369\n",
      "0.01839151324003059 0.41555879778904126 5.6792473793029785\n",
      "0.019276507307866352 0.44277999559293735 5.476776123046875\n",
      "0.01622014593857964 0.40538870777397257 6.035677433013916\n",
      "0.010968505092380083 0.4507471675050834 6.417087078094482\n",
      "0.005584514641670237 0.2685579770129362 5.838232040405273\n",
      "0.018318084801650435 0.4818437338956766 6.097972869873047\n",
      "0.017186353947416026 0.4079210303622413 5.594141960144043\n",
      "0.014246858781507547 0.4207553586083712 5.980581760406494\n",
      "0.03267826157065458 0.5651704229995095 6.052246570587158\n",
      "0.015631264514887513 0.31882025047965146 6.332162857055664\n",
      "0.012102302338201063 0.3622204620369456 6.172388553619385\n",
      "0.013041584121074727 0.3554367527940533 5.732439041137695\n",
      "0.041919405945083754 0.7162524281656883 6.024300575256348\n",
      "0.015989229629253632 0.3521581948451558 6.293938159942627\n",
      "0.009928322614473682 0.30322549712540275 6.1800055503845215\n",
      "0.015398082212555127 0.3682612723455683 6.359333515167236\n",
      "0.014642124959521996 0.28890478112984935 5.586442947387695\n",
      "0.023821057769603905 0.3777758992654291 5.705031394958496\n",
      "0.017218445447831732 0.32077121877209164 5.751089572906494\n",
      "0.02984226089976534 0.40699318458730843 6.23527193069458\n",
      "0.011399425723562936 0.2822220753040754 5.561037540435791\n",
      "0.012311597911373434 0.3620380661832978 6.213531970977783\n",
      "0.017359750195242298 0.4042093739017181 5.9724297523498535\n",
      "0.012115040905850572 0.38203834386101065 5.936286926269531\n",
      "0.018386892278770203 0.37815435966930144 6.091971397399902\n",
      "0.027860261161152013 0.516815414709228 5.542124271392822\n",
      "0.008161850155030669 0.29524989297229254 6.276317119598389\n",
      "0.02733289123224087 0.4291406982743996 5.481642723083496\n",
      "0.012999923220731534 0.3326800271790704 5.7353997230529785\n",
      "0.02640628815565596 0.4631735177072528 5.512732028961182\n",
      "0.02052966077469751 0.48879327105120013 6.077525615692139\n",
      "0.01694983993747898 0.29822864305334945 6.189267158508301\n",
      "0.025573038610934073 0.4097610229099272 6.1060895919799805\n",
      "0.02245836144857363 0.38529383444572807 5.491403102874756\n",
      "0.016339757740690602 0.3462231564813901 6.357864856719971\n",
      "0.01658156699796052 0.344409675146151 5.478465557098389\n",
      "0.01419018993046823 0.306690072331533 5.78387975692749\n",
      "0.019403626561975446 0.39301990178082113 5.907855033874512\n",
      "0.01877120076712889 0.37353693473403615 5.502561569213867\n",
      "0.025130132991017625 0.4598802016966496 6.264897346496582\n",
      "0.020906657333816484 0.4060190902385382 5.660223960876465\n",
      "0.02491403899564253 0.43129034041655384 6.1080451011657715\n",
      "0.018406227177328326 0.2938673098822567 5.790106296539307\n",
      "0.012160400334303453 0.3140868979152304 5.594112873077393\n",
      "0.031186111317398735 0.46656869642354043 5.571579456329346\n",
      "0.015324111438893194 0.29779487820449985 5.697493076324463\n",
      "0.025802785405169257 0.4052934988546561 5.564218044281006\n",
      "0.0163427839948508 0.39382994604840255 6.195410251617432\n",
      "0.02182929031803518 0.3964284088032147 5.5155792236328125\n",
      "0.018629124544080642 0.45161675089418846 5.591219902038574\n",
      "0.018321452591809253 0.4198541739385401 6.260832786560059\n",
      "0.013396510101490377 0.2865665672576198 5.670975685119629\n",
      "0.01240440477392154 0.404689045241455 6.382109642028809\n",
      "0.020590126112253516 0.4451831311103896 6.017794132232666\n",
      "0.01013172545321032 0.4203596746640756 6.505784034729004\n",
      "0.01154050143088288 0.36526964400388245 5.7764787673950195\n",
      "0.01282674413456633 0.2872965552093935 5.759327411651611\n",
      "0.01419598649996124 0.38651833785092726 6.415687561035156\n",
      "0.019013738211170854 0.33204186827984516 6.272407531738281\n",
      "0.014993634756995243 0.30246439702516775 5.470627784729004\n",
      "0.018812589819418295 0.40073675998020697 6.3756818771362305\n",
      "0.0326572253717694 0.5624918769036407 5.941256523132324\n",
      "0.020123350978914104 0.3327840680492174 5.590728759765625\n",
      "0.014166568620765726 0.3245430257159032 6.099825382232666\n",
      "0.014341745798039167 0.46280005721901457 6.322548866271973\n",
      "0.017871911738215947 0.45432081305182437 5.744575500488281\n",
      "0.016245345227756135 0.3649438258526265 5.560680866241455\n",
      "0.019911808956618458 0.4772657041321315 5.876446723937988\n",
      "0.0215962163438592 0.4439393949742182 5.830985069274902\n",
      "0.018889141551667682 0.4585381846149257 5.585273265838623\n",
      "0.013278043509334422 0.36396801524050115 6.435510635375977\n",
      "0.018000505243328142 0.5708626584180734 6.042367935180664\n",
      "0.020942062725895685 0.36231063348367853 6.149281024932861\n",
      "0.03286380157027433 0.6082893536105214 5.887552738189697\n",
      "0.01632646146314513 0.41540802592285875 6.433400630950928\n",
      "0.021294580710812257 0.3792665693744593 5.453332901000977\n",
      "0.02383761492290232 0.4425515696436512 6.201120853424072\n",
      "0.014857166925786883 0.5570804553761393 6.523340225219727\n",
      "0.018467123704052266 0.2980379338530275 5.576401233673096\n",
      "0.01884027892195409 0.38722582328418886 5.890878677368164\n",
      "0.02217098520507073 0.47896050161144704 5.407016277313232\n",
      "0.013800906120903296 0.33966942279693635 6.676492214202881\n",
      "tensor(6.0746, device='cuda:0')\n",
      "0.31563980431501315\n",
      "0.012636219910217452 0.33105934142472654 6.167668342590332\n",
      "0.013116685986991634 0.3342496299769392 5.6600518226623535\n",
      "0.012997706037734199 0.29489629246538074 6.137239456176758\n",
      "0.0107625712483333 0.3378195834045561 6.05239725112915\n",
      "0.010701447715922567 0.3023171930902072 6.049790859222412\n",
      "0.012274324548402618 0.336275131708757 5.500792503356934\n",
      "0.017595594098556343 0.41737809912438645 5.259646415710449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01186185656281727 0.46044892158997375 6.142352104187012\n",
      "0.017362675748413308 0.4070826453568049 5.434944152832031\n",
      "0.010351967362169692 0.2905875860442476 5.554415225982666\n",
      "0.006618976662148735 0.2829564738560935 6.042584419250488\n",
      "0.007278452316004279 0.25951796817505085 6.175039291381836\n",
      "0.011124389888925078 0.32000129539837974 5.65996789932251\n",
      "0.008650053757721262 0.26705171567973024 5.563077449798584\n",
      "0.013889344804152218 0.35277821763572775 5.62557315826416\n",
      "0.021017992544070656 0.3888289620612324 5.286975860595703\n",
      "0.019056165834338175 0.40148037363421674 5.932153701782227\n",
      "0.012852666573782513 0.3193530861934041 5.3893656730651855\n",
      "0.01674328915253991 0.40460952773421005 5.2937703132629395\n",
      "0.010696732550137764 0.36135023174296094 6.2090253829956055\n",
      "0.02314914698823749 0.36380338792560485 5.297841548919678\n",
      "0.014130475363407885 0.34981725631178656 6.075946807861328\n",
      "0.01277039780097729 0.33359558858278804 6.000683784484863\n",
      "0.024779531165011054 0.4602439875839924 5.032612323760986\n",
      "0.028384988676718615 0.6099758447953276 6.1055684089660645\n",
      "0.01805272509824348 0.37695363055864073 5.581811904907227\n",
      "0.018508885021273933 0.44565057078480913 5.301999092102051\n",
      "0.02532434208988944 0.5288333568156599 6.076108932495117\n",
      "0.01592182047033719 0.4478276419934144 6.057193279266357\n",
      "0.011693375204682947 0.3014867700699852 5.6084160804748535\n",
      "0.02085352108930924 0.44506288001476146 5.460099220275879\n",
      "0.018700491335599 0.4257502584104487 5.99208402633667\n",
      "0.03247197819640206 0.44443489629014066 5.290330410003662\n",
      "0.016212230184173677 0.43124522940227317 5.5727925300598145\n",
      "0.01119713308586023 0.34083017602009436 6.170322895050049\n",
      "0.017232651935476666 0.38597159963038774 5.651939868927002\n",
      "0.02037449663249305 0.34679519756659466 5.497073173522949\n",
      "0.011619659303584188 0.39330574420427417 5.98972749710083\n",
      "0.01702900584823979 0.33956237719399435 5.558465957641602\n",
      "0.020589057797098766 0.4367332773531556 5.411084175109863\n",
      "0.015348595265814098 0.41852340570992363 6.242502212524414\n",
      "0.016306415632137788 0.5097788690700372 5.266392230987549\n",
      "0.02709464395483679 0.4489181067905184 6.02343225479126\n",
      "0.03177347285251203 0.5393559083131991 5.928032875061035\n",
      "0.022549946151046062 0.5343221577411283 6.0863118171691895\n",
      "0.01562789882515795 0.5548081209817065 5.542692184448242\n",
      "0.019911675879617154 0.45565928145772494 6.3881402015686035\n",
      "0.025786812628310413 0.5795452017743351 5.890416622161865\n",
      "0.01613782440255527 0.330302976572931 5.342670440673828\n",
      "0.028462590550724455 0.516461153613399 5.857118129730225\n",
      "0.01655623676832501 0.41395405419902875 5.200651168823242\n",
      "0.015100507860761083 0.575101732497185 6.446033477783203\n",
      "0.02478356754403656 0.540263536598868 5.919134140014648\n",
      "0.04004098226356771 0.6582050588602782 6.102538108825684\n",
      "0.025233542493295145 0.37119176643430773 5.602279186248779\n",
      "0.022004314449489353 0.42374930902217267 6.009511947631836\n",
      "0.019704776726751663 0.40824202235299345 5.395589351654053\n",
      "0.010689599620320732 0.3310475864932348 6.275350570678711\n",
      "0.021511340042355703 0.42085776863854696 5.795346260070801\n",
      "0.018482301747128286 0.3592225656871805 5.6223297119140625\n",
      "0.01657612118642247 0.3391804222556583 5.520631313323975\n",
      "0.009986793423689109 0.2940017545098454 6.41913366317749\n",
      "0.0288307648093156 0.44753137955170247 5.495937824249268\n",
      "0.02798903486108484 0.6239018642728883 6.318766117095947\n",
      "0.024907510787509288 0.5016883043172264 5.3761701583862305\n",
      "0.015968153427041124 0.3474424123337778 5.582573890686035\n",
      "0.027178743767019467 0.5509355950949046 6.219269275665283\n",
      "0.028293194889598486 0.6021652489152189 6.130136013031006\n",
      "0.031129357628730744 0.4661820970322917 5.9279890060424805\n",
      "0.022675468934920683 0.4796825625672663 6.3530683517456055\n",
      "0.015808623948489718 0.32743705928807987 6.284923553466797\n",
      "0.025427921641950485 0.6307127394967469 5.1696906089782715\n",
      "0.019281794514940664 0.35981350003789303 5.519872188568115\n",
      "0.011401850392498684 0.3509196787286956 5.349216938018799\n",
      "0.009825772024205669 0.37504404086336113 6.296283721923828\n",
      "0.02479174748406136 0.44823586995797676 5.581888675689697\n",
      "0.0185127603978898 0.36384209706515186 5.527603626251221\n",
      "0.013525411811459258 0.3609749067497006 5.727730751037598\n",
      "0.025480531744606724 0.3813463439700233 5.910674571990967\n",
      "0.01445567865713705 0.34157595075350383 5.725426197052002\n",
      "0.038979902024035624 0.6007236094239063 5.295292854309082\n",
      "0.018489055014568666 0.4132674861892683 6.165633678436279\n",
      "0.01763577982773938 0.4661666823616522 6.150997638702393\n",
      "0.020515649885067985 0.4604005946017227 6.105865955352783\n",
      "0.01599245316048889 0.3645391604272412 6.191533088684082\n",
      "0.0294490884568316 0.6031082057984216 5.946444511413574\n",
      "0.02552608491118752 0.46285574286188563 6.190695285797119\n",
      "0.021884845777567295 0.5538477990464412 5.339146137237549\n",
      "0.015488036070560594 0.3466757199028039 5.574763774871826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-257ee7664189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/PhD_Projects/nlpclass/nlpclass/models/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             decoder_output, decoder_hidden, context, weights = self.decoder(\n\u001b[0;32m--> 194\u001b[0;31m                 decoder_input, decoder_hidden, encoder_output, context)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/PhD_Projects/nlpclass/nlpclass/models/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_output, context)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(25)):\n",
    "    for batch in data_loaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        logits = translation_model(batch)\n",
    "        loss = calc_loss(logits, batch['target'], criterion)\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_norm = 0\n",
    "        for p in translation_model.encoder.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            encoder_norm += param_norm.item() ** 2\n",
    "        decoder_norm = 0\n",
    "        for p in translation_model.decoder.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            decoder_norm += param_norm.item() ** 2\n",
    "            \n",
    "        print(encoder_norm, decoder_norm, loss.item())\n",
    "            \n",
    "        clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                                   translation_model.parameters()), 5.0)\n",
    "        \n",
    "        optimizer.step() \n",
    "    original, translation = evaluate(translation_model, data, data_loaders, dataset_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08687826695566427\n",
      "[('so i think that we have to the', 'today im going to show you an electric vehicle that weighs less than a bicycle that you can carry with you anywhere that you can charge off a normal wall outlet in 15 minutes and you can run it for 1 000 kilometers on about a dollar of electricity .'), ('and we can the the .', 'can farm raised salmon be organic when its feed has nothing to do with its natural diet even if the feed itself is supposedly organic and the fish themselves are packed tightly in pens swimming in their own filth ?')]\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.greedy(x), data['train'])\n",
    "print(bleu_eval(original, translations))\n",
    "print(list(zip(translations[0:2], original[0:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-04a482f71426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_to_translations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_to_translations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/PhD_Projects/nlpclass/nlpclass/models/models.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             enocoder_output_one = encoder_output[sentence, :, :].unsqueeze(\n\u001b[0m\u001b[1;32m    357\u001b[0m                 dim=0)\n\u001b[1;32m    358\u001b[0m             \u001b[0mdecoder_hidden_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "original = output_to_translations(x['target'], data['train'])\n",
    "translations = output_to_translations(translation_model.beam_search(x), data['train'])\n",
    "print(bleu_eval(original, translations))\n",
    "print(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "input_seq = x['input']\n",
    "target_seq = x['target']\n",
    "input_length = x['input_length']\n",
    "target_length = x['target_length']\n",
    "\n",
    "encoded_input, hidden = translation_model.encoder.forward(input_seq, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6125, -0.5094,  0.1136,  ...,  0.2936, -0.0332, -0.5566],\n",
       "        [-0.2966, -0.5157,  0.0630,  ...,  0.6734,  0.0263, -0.9648],\n",
       "        [ 0.3257, -1.1140,  0.3586,  ...,  0.7310, -0.7097, -0.5748],\n",
       "        ...,\n",
       "        [-0.3053, -0.1229,  0.2143,  ..., -0.5936, -0.5418, -0.0655],\n",
       "        [-0.2868, -0.2641,  0.6324,  ..., -0.5235, -0.4746, -0.1487],\n",
       "        [-0.0230, -0.0655,  0.1169,  ..., -0.4210,  0.3063,  0.4462]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(9.9659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(8.5004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(7.8864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.6769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.8524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.8219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.5111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.7274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.8735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.7627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.8793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.7171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(data_loaders['train']))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-2)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(x)\n",
    "    loss = calc_loss(logits, x['target'], criterion)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    translation_model.encoder.embedding.weight.grad[data['train'].input_lang.pretrained_inds] = 0\n",
    "    translation_model.decoder.embedding.weight.grad[data['train'].output_lang.pretrained_inds] = 0\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, data_loaders, dataset_type='dev', max_batches=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        original_strings = []\n",
    "        translated_strings = []\n",
    "        for i, batch in enumerate(data_loaders[dataset_type]):\n",
    "            if i > max_batches:\n",
    "                break\n",
    "            logits = translation_model(batch)\n",
    "            epoch_loss = calc_loss(logits, batch['target'], criterion)\n",
    "            original = output_to_translations(batch['target'], data['train'])\n",
    "            translations = output_to_translations(model.greedy(batch), data['train'])\n",
    "            original_strings.extend(original)\n",
    "            translated_strings.extend(translations)\n",
    "        bleu = bleu_eval(original_strings, translated_strings)\n",
    "        model.train()\n",
    "        print(epoch_loss)\n",
    "        print(bleu)\n",
    "        \n",
    "        return original_strings, translated_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, translation_model.parameters()), 1e-3)\n",
    "weight = torch.ones(translation_model.decoder.output_size).to(model_config.device)\n",
    "weight[model_config.PAD_token] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34410a3c73c44d89f2e6cddd5d165c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1389), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.1450, device='cuda:0')\n",
      "0.0008142795832761162\n",
      "tensor(9.0464, device='cuda:0')\n",
      "0.0033784249080402885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bf6d2f87ea3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#evaluate(translation_model, data, data_loaders, dataset_type='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/PhD_Projects/nlpclass/nlpclass/models/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         decoder_outputs = Variable(torch.zeros(\n\u001b[0;32m--> 166\u001b[0;31m             model_config.max_length + 1, batch_size, self.decoder.output_size)).to(model_config.device)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm_notebook(data_loaders['train'])):\n",
    "    if i % 500 == 0:\n",
    "        evaluate(translation_model, data, data_loaders)\n",
    "        #evaluate(translation_model, data, data_loaders, dataset_type='train')\n",
    "    optimizer.zero_grad()\n",
    "    logits = translation_model(batch)\n",
    "    loss = calc_loss(logits, batch['target'], criterion)\n",
    "    loss.backward()\n",
    "    translation_model.encoder.embedding.weight.grad[data['train'].input_lang.pretrained_inds] = 0\n",
    "    translation_model.decoder.embedding.weight.grad[data['train'].output_lang.pretrained_inds] = 0\n",
    "    clip_grad_norm_(filter(lambda p: p.requires_grad,\n",
    "                               translation_model.parameters()), model_config.grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_strings, translated_strings = evaluate(translation_model, data, data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball be later but of volumes all. hope. answer. only were',\n",
       " 'be this. 65. know what must truth. i most',\n",
       " 'games be practical everybody pins your needles all forward careful were',\n",
       " 'i moment. it attack. be sorrows. ten.',\n",
       " 'canadian be appointment careful laugh. no truth. something time were',\n",
       " 'beautiful cupboard. hot. wine these',\n",
       " 'much. but how bitter really dishonesty fed recently',\n",
       " 'applied all else fed to climate husbands. matter. key',\n",
       " 'but they answer. perfect. today. tape',\n",
       " 'i is invented i situation getting powers your few']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the',\n",
       " 'i you you to the']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = translation_model.greedy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in predictions.cpu().numpy():\n",
    "    decoded_words = []\n",
    "    for elem in row[1:]:\n",
    "        decoded_words.append(data['train']['output_lang'].index2word[elem])\n",
    "        if elem == model_config.EOS_token:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = Variable(torch.LongTensor([model_config.SOS_token] * 8)).to(model_config.device)\n",
    "yo = torch.stack((yo, topi.squeeze(), topi.squeeze()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_range = torch.autograd.Variable(torch.LongTensor(np.repeat([2], len(x['input_length'])))).to(model_config.device)\n",
    "mask = seq_range < x['input_length']\n",
    "loss = -torch.gather(decoder_output, dim=1, index=input_var.unsqueeze(1)).squeeze() * mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4193, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.sum() / torch.sum(loss > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss > 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_output, encoder_hidden = encoder(x['input'], x['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = None\n",
    "if decoder.attention:\n",
    "    context = Variable(torch.zeros(encoder_output.size(0), encoder_output.size(2))).unsqueeze(1).to(model_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden, context, weights = decoder(input_var, encoder_hidden, encoder_output, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, criterion):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item() * \\\n",
    "            len(batch['label']) / len(train_loader.dataset)\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30a2084915f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-df8527ea006d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_loader, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(translation_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_model(translation_model, optimizer, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
